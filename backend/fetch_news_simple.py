#!/usr/bin/env python3
"""
æ–°é—»èšåˆæŠ“å–è„šæœ¬ - ç®€åŒ–ç‰ˆ
åªä½¿ç”¨ç¨³å®šçš„æ–°é—»æº
"""

import json
import requests
from datetime import datetime
from pathlib import Path

DATA_DIR = Path(__file__).parent.parent / "data"
DATA_DIR.mkdir(exist_ok=True)

PROXY = {'http': 'http://127.0.0.1:1082', 'https': 'http://127.0.0.1:1082'}
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}

def is_shanghai_relevant(title):
    """åˆ¤æ–­æ˜¯å¦ä¸å˜‰å®šç›¸å…³"""
    text = title.lower()
    jiading_keywords = ['å˜‰å®š', 'å—ç¿”', 'æ±Ÿæ¡¥', 'å®‰äº­', 'é©¬é™†', 'å¤–å†ˆ', 'å¾è¡Œ', 'åäº­', 'èŠå›­', 'æ–°æˆè·¯', 'çœŸæ–°', 'å·æ¡¥']
    season_keywords = ['ç«‹æ˜¥', 'é›¨æ°´', 'æƒŠè›°', 'æ˜¥åˆ†', 'æ¸…æ˜', 'è°·é›¨']
    community_keywords = ['ç¤¾åŒº', 'è¡—é“', 'å±…å§”ä¼š', 'ç‰©ä¸š', 'å…»è€', 'åŠ è£…ç”µæ¢¯']
    
    has_jiading = any(kw in text for kw in jiading_keywords)
    has_season = any(kw in text for kw in season_keywords)
    has_community = any(kw in text for kw in community_keywords)
    
    return {
        'jiading': has_jiading,
        'season': has_season,
        'community': has_community,
        'score': int(has_jiading) * 3 + int(has_season) * 2 + int(has_community) * 1
    }

def fetch_shanghai():
    """æŠ“å–ä¸Šæµ·æ–°é—»"""
    items = []
    
    # 1. æ–°æµªä¸Šæµ·
    try:
        url = "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2515&k=&num=30&r=0.123"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('result') and data['result'].get('data'):
                for item in data['result']['data']:
                    title = item.get('title', '').strip()
                    relevance = is_shanghai_relevant(title)
                    tags = []
                    if relevance['jiading']: tags.append('ğŸ ')
                    if relevance['season']: tags.append('ğŸŒ¸')
                    if relevance['community']: tags.append('ğŸ‘¥')
                    
                    items.append({
                        "title": f"{' '.join(tags)} {title}" if tags else title,
                        "link": item.get('url', ''),
                        "summary": "æ–°æµªä¸Šæµ·",
                        "source": "æ–°æµªä¸Šæµ·",
                        "time": item.get('time', '')[5:16] if len(item.get('time', '')) > 16 else datetime.now().strftime("%m-%d"),
                        "isNew": True,
                        "score": relevance['score']
                    })
        print(f"âœ“ æ–°æµªä¸Šæµ·: {len([i for i in items if i['source']=='æ–°æµªä¸Šæµ·'])} æ¡")
    except Exception as e:
        print(f"âœ— æ–°æµªä¸Šæµ·: {e}")
    
    # 2. å˜‰å®šç²¾é€‰
    items.extend([
        {"title": "ğŸ  å˜‰å®šæ–°åŸå»ºè®¾æé€Ÿï¼Œå¤šä¸ªé‡å¤§é¡¹ç›®é›†ä¸­å¼€å·¥", "link": "https://www.jiading.gov.cn/", "summary": "å˜‰å®šå‘å¸ƒ", "source": "å˜‰å®šå‘å¸ƒ", "time": datetime.now().strftime("%m-%d"), "isNew": True, "score": 3},
        {"title": "ğŸ‘¥ å—ç¿”é•‡åŠ è£…ç”µæ¢¯å·¥ç¨‹åˆæœ‰æ–°è¿›å±•", "link": "https://www.jiading.gov.cn/", "summary": "å—ç¿”é•‡", "source": "å—ç¿”é•‡", "time": datetime.now().strftime("%m-%d"), "isNew": True, "score": 3},
    ])
    
    # å»é‡æ’åº
    seen = set()
    unique = []
    for item in items:
        if item['title'] not in seen:
            seen.add(item['title'])
            unique.append(item)
    unique.sort(key=lambda x: x.get('score', 0), reverse=True)
    return unique

def fetch_world():
    """æŠ“å–ä¸–ç•Œæ–°é—»"""
    items = []
    try:
        url = "https://www.reddit.com/r/worldnews/new.json?limit=10"
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10, proxies=PROXY)
        if response.status_code == 200:
            data = response.json()
            for post in data['data']['children']:
                items.append({
                    "title": post['data']['title'],
                    "link": "https://reddit.com" + post['data']['permalink'],
                    "summary": f"â¬†ï¸ {post['data'].get('score', 0)}",
                    "source": "Reddit",
                    "time": datetime.now().strftime("%m-%d"),
                    "isNew": True
                })
        print(f"âœ“ Reddit: {len(items)} æ¡")
    except Exception as e:
        print(f"âœ— Reddit: {e}")
    return items

def fetch_ai():
    """æŠ“å–AIæ–°é—»"""
    items = []
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=10, proxies=PROXY)
        top_ids = response.json()[:8]
        for story_id in top_ids:
            try:
                story = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json", timeout=5, proxies=PROXY).json()
                if story and story.get('title'):
                    items.append({
                        "title": story['title'],
                        "link": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                        "summary": f"â­ {story.get('score', 0)} points",
                        "source": "Hacker News",
                        "time": datetime.now().strftime("%m-%d"),
                        "isNew": True
                    })
            except:
                continue
        print(f"âœ“ Hacker News: {len(items)} æ¡")
    except Exception as e:
        print(f"âœ— Hacker News: {e}")
    return items

def fetch_stocks():
    """ç¾è‚¡æ–°é—»"""
    return [
        {"title": "ğŸš€ RKLB Rocket Lab æœ€æ–°å‘å°„ä»»åŠ¡", "link": "https://www.rocketlabusa.com/news/", "summary": "å®˜æ–¹æ–°é—»", "source": "Rocket Lab", "time": datetime.now().strftime("%m-%d"), "isNew": True},
        {"title": "âš¡ TSLA ç‰¹æ–¯æ‹‰æŠ•èµ„è€…å…³ç³»", "link": "https://ir.tesla.com/", "summary": "è´¢æŠ¥å…¬å‘Š", "source": "Tesla", "time": datetime.now().strftime("%m-%d"), "isNew": True},
        {"title": "ğŸ“Š PLTR Palantir å•†ä¸šåŠ¨æ€", "link": "https://investors.palantir.com/news/", "summary": "åˆåŒæ›´æ–°", "source": "Palantir", "time": datetime.now().strftime("%m-%d"), "isNew": True},
    ]

def fetch_policy():
    """æ”¿ç­–æ–°é—»"""
    return [
        {"title": "ğŸ‡¨ğŸ‡³ å›½åŠ¡é™¢å‘å¸ƒæœªæ¥äº§ä¸šåˆ›æ–°å‘å±•æ„è§", "link": "http://www.gov.cn", "summary": "å‰ç»å¸ƒå±€å…­å¤§æ–¹å‘", "source": "å›½åŠ¡é™¢", "time": "02-04", "isNew": True},
        {"title": "ğŸ­ å·¥ä¿¡éƒ¨ï¼šåŠ å¿«åˆ¶é€ ä¸šæ•°å­—åŒ–è½¬å‹", "link": "https://www.miit.gov.cn", "summary": "æ¨åŠ¨é«˜ç«¯åŒ–æ™ºèƒ½åŒ–", "source": "å·¥ä¿¡éƒ¨", "time": "02-03", "isNew": False},
        {"title": "ğŸ’° å¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹", "link": "http://www.pbc.gov.cn", "summary": "é‡Šæ”¾èµ„é‡‘1ä¸‡äº¿å…ƒ", "source": "å¤®è¡Œ", "time": "02-01", "isNew": False},
    ]

def main():
    print(f"\nâ° {datetime.now().strftime('%H:%M:%S')} - å¼€å§‹æŠ“å–\n")
    
    news_data = {
        "shanghai": fetch_shanghai(),
        "world": fetch_world(),
        "ai": fetch_ai(),
        "stocks": fetch_stocks(),
        "policy": fetch_policy()
    }
    
    # ä¿å­˜
    with open(DATA_DIR / "news.json", "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    
    import shutil
    shutil.copy(DATA_DIR / "news.json", DATA_DIR.parent / "frontend" / "data.json")
    shutil.copy(DATA_DIR / "news.json", DATA_DIR.parent / "data.json")
    
    print(f"\nâœ… å®Œæˆ! æ€»è®¡ {sum(len(v) for v in news_data.values())} æ¡")
    for k, v in news_data.items():
        print(f"  {k}: {len(v)}æ¡")

if __name__ == "__main__":
    main()
