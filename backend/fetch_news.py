#!/usr/bin/env python3
"""
æ–°é—»èšåˆæŠ“å–è„šæœ¬ - ä½¿ç”¨æœ¬åœ°å¯è®¿é—®æº
"""

import json
import requests
from datetime import datetime
from pathlib import Path
import html
import re

DATA_DIR = Path(__file__).parent.parent / "data"
DATA_DIR.mkdir(exist_ok=True)

def fetch_html_news():
    """ç›´æ¥æŠ“å–ç½‘é¡µæ–°é—» - ç»•è¿‡ RSS"""
    
    news_data = {
        "shanghai": [],
        "stocks": [],
        "policy": [],
        "world": [],
        "ai": []
    }
    
    print("ğŸ“° å¼€å§‹æŠ“å–æ–°é—»...\n")
    
    # 1. ä¸–ç•Œæ–°é—» - BBC ä¸­æ–‡
    try:
        print("ğŸŒ æŠ“å– BBC æ–°é—»...")
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        
        # BBC ä¸­æ–‡
        response = requests.get("https://www.bbc.com/zhongwen/simp/world", 
                              headers=headers, timeout=10)
        response.raise_for_status()
        
        # ç®€å•æå–æ ‡é¢˜
        titles = re.findall(r'class="[^"]*title[^"]*"[^>]*><a[^h]*href="([^"]+)"[^>]*>([^<]+)<', response.text)
        
        for i, (link, title) in enumerate(titles[:5]):
            clean_title = html.unescape(title.strip())
            if clean_title and len(clean_title) > 10:
                news_data["world"].append({
                    "title": clean_title,
                    "link": "https://www.bbc.com" + link if link.startswith('/') else link,
                    "summary": "",
                    "source": "BBC",
                    "time": datetime.now().strftime("%m-%d"),
                    "isNew": i < 3
                })
        print(f"  âœ“ BBC: {len(news_data['world'])} æ¡")
    except Exception as e:
        print(f"  âœ— BBC: {str(e)[:40]}")
    
    # 2. AI æ–°é—» - Hacker News (é€šè¿‡ API)
    try:
        print("ğŸ¤– æŠ“å– Hacker News...")
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", 
                              timeout=10)
        top_ids = response.json()[:8]
        
        for i, story_id in enumerate(top_ids):
            try:
                story_resp = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                                        timeout=5)
                story = story_resp.json()
                if story and story.get('title'):
                    news_data["ai"].append({
                        "title": story['title'],
                        "link": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                        "summary": f"{story.get('score', 0)} points",
                        "source": "Hacker News",
                        "time": datetime.now().strftime("%m-%d"),
                        "isNew": i < 4
                    })
            except:
                continue
        print(f"  âœ“ Hacker News: {len(news_data['ai'])} æ¡")
    except Exception as e:
        print(f"  âœ— Hacker News: {str(e)[:40]}")
    
    # 3. ä¸Šæµ·æ–°é—» - ä½¿ç”¨é™æ€ç¤ºä¾‹ï¼ˆéœ€è¦æ‰‹åŠ¨æ›´æ–°ï¼‰
    print("ğŸ™ï¸ ä¸Šæµ·æ–°é—» - ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
    news_data["shanghai"] = [
        {
            "title": "ä¸Šæµ·å¸‚å‘å¸ƒæ–°ä¸€è½®ä¼˜åŒ–è¥å•†ç¯å¢ƒè¡ŒåŠ¨æ–¹æ¡ˆ",
            "link": "https://www.shanghai.gov.cn",
            "summary": "ä¸Šæµ·å¸‚æ”¿åºœå‘å¸ƒ7.0ç‰ˆä¼˜åŒ–è¥å•†ç¯å¢ƒè¡ŒåŠ¨æ–¹æ¡ˆ",
            "source": "ä¸Šæµ·å‘å¸ƒ",
            "time": "02-03",
            "isNew": True
        },
        {
            "title": "å˜‰å®šæ–°åŸå»ºè®¾æé€Ÿï¼Œå¤šä¸ªé‡å¤§é¡¹ç›®å¼€å·¥",
            "link": "https://www.jiading.gov.cn",
            "summary": "å˜‰å®šåŒºæ¨åŠ¨æ–°åŸå»ºè®¾ï¼Œèšç„¦ç§‘æŠ€åˆ›æ–°",
            "source": "å˜‰å®šå‘å¸ƒ",
            "time": "02-03",
            "isNew": True
        },
    ]
    print(f"  âœ“ ä¸Šæµ·æ–°é—»: {len(news_data['shanghai'])} æ¡")
    
    # 4. å›½å†…æ”¿ç­–
    print("ğŸ‡¨ğŸ‡³ å›½å†…æ”¿ç­– - ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
    news_data["policy"] = [
        {
            "title": "å›½åŠ¡é™¢å‘å¸ƒå…³äºæ¨åŠ¨æœªæ¥äº§ä¸šåˆ›æ–°å‘å±•çš„å®æ–½æ„è§",
            "link": "http://www.gov.cn",
            "summary": "å‰ç»å¸ƒå±€æœªæ¥äº§ä¸šï¼Œé‡ç‚¹æ¨è¿›å…­å¤§æ–¹å‘",
            "source": "ä¸­å›½æ”¿åºœç½‘",
            "time": "02-03",
            "isNew": True
        },
        {
            "title": "å·¥ä¿¡éƒ¨ï¼šåŠ å¿«åˆ¶é€ ä¸šæ•°å­—åŒ–è½¬å‹",
            "link": "https://www.miit.gov.cn",
            "summary": "æ¨åŠ¨åˆ¶é€ ä¸šé«˜ç«¯åŒ–ã€æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‘å±•",
            "source": "å·¥ä¿¡éƒ¨",
            "time": "02-02",
            "isNew": False
        },
    ]
    print(f"  âœ“ å›½å†…æ”¿ç­–: {len(news_data['policy'])} æ¡")
    
    # 5. ç¾è‚¡æ–°é—» - ä½¿ç”¨ç»¼åˆé‡‘èæ–°é—»æº
    try:
        print("ğŸ“ˆ æŠ“å–ç¾è‚¡æ–°é—»...")
        
        # å°è¯•æŠ“å– Investing.com æˆ– MarketWatch
        try:
            # MarketWatch RSS
            url = "https://rsshub.app/marketwatch/realtime"
            response = requests.get(url, headers=headers, timeout=15,
                                   proxies={'http': 'http://127.0.0.1:1082', 'https': 'http://127.0.0.1:1082'})
            
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                for entry in feed.entries[:5]:
                    title = html.unescape(entry.get("title", "")).strip()
                    # åªä¿ç•™ä¸ä½ çš„æŒä»“ç›¸å…³çš„è‚¡ç¥¨æ–°é—»
                    relevant = any(s in title.upper() for s in ["TESLA", "TSLA", "ROCKET LAB", "RKLB", 
                                                                "QUANTUM", "QS", "PALANTIR", "PLTR"])
                    if relevant or len(news_data["stocks"]) < 3:
                        news_data["stocks"].append({
                            "title": title[:60] + "..." if len(title) > 60 else title,
                            "link": entry.get("link", ""),
                            "summary": "ç¾è‚¡å¸‚åœºæ–°é—»",
                            "source": "MarketWatch",
                            "time": format_time(entry.get("published", "")),
                            "isNew": True
                        })
        except:
            pass
        
        # å¦‚æœæŠ“ä¸åˆ°ï¼Œä½¿ç”¨é¢„è®¾çš„é«˜è´¨é‡é“¾æ¥
        if len(news_data["stocks"]) < 3:
            # æ·»åŠ ä¸€äº›é‡è¦çš„è‚¡ç¥¨æ–°é—»æº
            news_data["stocks"].extend([
                {
                    "title": "ğŸš€ RKLB Rocket Lab æœ€æ–°å‘å°„ä»»åŠ¡åŠ¨æ€",
                    "link": "https://www.rocketlabusa.com/news/",
                    "summary": "Rocket Lab å‘å°„ä»»åŠ¡ä¸å…¬å¸æ–°é—»",
                    "source": "Rocket Lab Official",
                    "time": datetime.now().strftime("%m-%d"),
                    "isNew": True
                },
                {
                    "title": "âš¡ TSLA ç‰¹æ–¯æ‹‰æœ€æ–°è´¢æŠ¥ä¸äº§å“åŠ¨æ€",
                    "link": "https://ir.tesla.com/",
                    "summary": "ç‰¹æ–¯æ‹‰æŠ•èµ„è€…å…³ç³»ä¸æ–°é—»å‘å¸ƒ",
                    "source": "Tesla IR",
                    "time": datetime.now().strftime("%m-%d"),
                    "isNew": True
                },
                {
                    "title": "ğŸ”‹ QS QuantumScape å›ºæ€ç”µæ± ç ”å‘è¿›å±•",
                    "link": "https://www.quantumscape.com/news/",
                    "summary": "QuantumScape æŠ€æœ¯çªç ´ä¸ä¸šåŠ¡è¿›å±•",
                    "source": "QuantumScape",
                    "time": datetime.now().strftime("%m-%d"),
                    "isNew": True
                },
                {
                    "title": "ğŸ“Š PLTR Palantir æ”¿åºœä¸ä¼ä¸šåˆåŒæ›´æ–°",
                    "link": "https://investors.palantir.com/news/",
                    "summary": "Palantir å•†ä¸šåŠ¨æ€ä¸æ–°é—»",
                    "source": "Palantir IR",
                    "time": datetime.now().strftime("%m-%d"),
                    "isNew": True
                },
            ])
        
        print(f"  âœ“ ç¾è‚¡: {len(news_data['stocks'])} æ¡")
    except Exception as e:
        print(f"  âœ— ç¾è‚¡: {str(e)[:40]}")
    
    # ä¿å­˜
    output_file = DATA_DIR / "news.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡
    print("\n" + "="*50)
    print("âœ… æŠ“å–å®Œæˆ!")
    total = sum(len(v) for v in news_data.values())
    print(f"   æ€»è®¡: {total} æ¡æ–°é—»")
    for k, v in news_data.items():
        print(f"   {k}: {len(v)} æ¡")
    print(f"\nğŸ’¾ å·²ä¿å­˜: {output_file}")
    
    return news_data

if __name__ == "__main__":
    fetch_html_news()
