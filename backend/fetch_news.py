import json
import os
import re
import random
import requests
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor

# ================= é…ç½®åŒºåŸŸ =================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}

RSS_SOURCES = {
    # ğŸ™ï¸ ä¸Šæµ·æœ¬åœ° (æ¢æˆäº†æ›´ç¨³å®šçš„â€œä¸­æ–°ç½‘-ä¸Šæµ·â€å’Œâ€œä¸œæ–¹ç½‘â€)
    'shanghai': [
        'http://www.sh.chinanews.com/rss/scroll-news.xml',  # ä¸­æ–°ç½‘ä¸Šæµ· (éå¸¸ç¨³å®š)
        'https://www.shobserver.com/rss/index.html',         # ä¸Šè§‚æ–°é—»
    ],
    # ğŸ¤– ç§‘æŠ€
    'tech': [
        'https://www.36kr.com/feed',
        'https://www.ifanr.com/feed',
    ],
    # ğŸ“ˆ ç¾è‚¡
    'us_stocks': [
        'https://feed.wallstreetcn.com/feed/live',
        'https://www.gelonghui.com/rss_feed.xml',
    ],
    # ğŸ‡¨ğŸ‡³ æ”¿ç­–
    'policy': [
        'http://www.news.cn/politics/news.xml',
        'http://www.chinanews.com/rss/gn.xml',
    ]
}

DEFAULT_IMAGES = {
    'shanghai': ["https://images.unsplash.com/photo-1474181487882-5abf3f0ba6c2?w=600&q=80"],
    'tech': ["https://images.unsplash.com/photo-1518770660439-4636190af475?w=600&q=80"],
    'us_stocks': ["https://images.unsplash.com/photo-1611974765270-ca1258822981?w=600&q=80"],
    'policy': ["https://images.unsplash.com/photo-1532375810709-75b1da00537c?w=600&q=80"]
}

def parse_rss_feed(url, category):
    print(f"æ­£åœ¨æŠ“å– [{category}]: {url}")
    news_items = []
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=20)
        # è‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œæˆ–è€…ç›´æ¥ç”¨ content (äºŒè¿›åˆ¶) äº¤ç»™ XML è§£æå™¨å¤„ç†ï¼Œé˜²æ­¢ GBK ä¹±ç 
        content = response.content
        
        try:
            root = ET.fromstring(content)
        except:
            # å¦‚æœæ ‡å‡†è§£æå¤±è´¥ï¼Œå°è¯•è§£ç åæ‰‹åŠ¨ä¿®å¤
            try:
                text = response.content.decode('utf-8')
            except:
                try:
                    text = response.content.decode('gbk') # å°è¯• GBK
                except:
                    text = response.text
            
            # ç§»é™¤å¯èƒ½å¯¼è‡´æŠ¥é”™çš„ç‰¹æ®Šç¬¦å·
            text = text.replace('&', '&amp;')
            root = ET.fromstring(text)

        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        for item in items[:20]:
            try:
                title = item.find('title').text if item.find('title') is not None else "æ— æ ‡é¢˜"
                link = item.find('link').text if item.find('link') is not None else ""
                
                # æè¿°å¤„ç†
                description = ""
                desc_tag = item.find('description') or item.find('content:encoded')
                if desc_tag is not None and desc_tag.text:
                    clean_text = re.sub(r'<[^>]+>', '', desc_tag.text)
                    description = clean_text[:100] + "..."

                # æ—¶é—´å¤„ç†
                pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                if pub_date: pub_date = pub_date[:16]

                # å›¾ç‰‡æå–
                image = ""
                if desc_tag is not None and desc_tag.text:
                    img_match = re.search(r'src="(http[^"]+\.(jpg|png|jpeg|webp))"', desc_tag.text)
                    if img_match: image = img_match.group(1)
                
                if not image:
                    image = random.choice(DEFAULT_IMAGES.get(category, DEFAULT_IMAGES['tech']))

                # æ¥æºæ ‡è®°
                source_name = "ç½‘ç»œæ–°é—»"
                if "chinanews" in url: source_name = "ä¸­å›½æ–°é—»ç½‘"
                elif "shobserver" in url: source_name = "ä¸Šè§‚æ–°é—»"
                elif "36kr" in url: source_name = "36æ°ª"
                elif "wallstreet" in url: source_name = "åå°”è¡—è§é—»"
                elif "news.cn" in url: source_name = "æ–°åç½‘"

                news_items.append({
                    "title": title.strip(),
                    "link": link,
                    "time": pub_date,
                    "source": source_name,
                    "image": image,
                    "summary": description,
                    "category": category
                })
            except:
                continue
    except Exception as e:
        print(f"âŒ {url} å‡ºé”™: {e}")
        
    return news_items

def fetch_all_news():
    all_news = {"shanghai":[], "tech":[], "us_stocks":[], "policy":[]}
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        tasks = [executor.submit(parse_rss_feed, url, cat) for cat, urls in RSS_SOURCES.items() for url in urls]
        for future in tasks:
            items = future.result()
            if items:
                all_news[items[0]['category']].extend(items)

    # === å…œåº•æœºåˆ¶ï¼šå¦‚æœæŸä¸ªåˆ†ç±»æ˜¯ç©ºçš„ï¼ŒåŠ ä¸€æ¡â€œå‡æ–°é—»â€æç¤º ===
    for cat in all_news:
        if not all_news[cat]:
            all_news[cat].append({
                "title": "æ­£åœ¨è·å–æœ€æ–°èµ„è®¯...",
                "link": "#",
                "time": "åˆšåˆš",
                "source": "ç³»ç»Ÿæç¤º",
                "image": random.choice(DEFAULT_IMAGES.get(cat, DEFAULT_IMAGES['tech'])),
                "summary": "è¯¥æ¿å—çš„æ–°é—»æºæ­£åœ¨æ›´æ–°ä¸­ï¼Œè¯·ç¨ååˆ·æ–°é¡µé¢æŸ¥çœ‹ã€‚",
                "category": cat
            })

    output_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=2)
    print("âœ… æ›´æ–°å®Œæˆ")

if __name__ == "__main__":
    fetch_all_news()
