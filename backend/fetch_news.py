#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»æŠ“å–è„šæœ¬ - ä¸“ä¸º News Dashboard å‰ç«¯è®¾è®¡
è§£å†³ï¼šRSSæŠ“å–å¤±è´¥ã€æ— å›¾ç‰‡ã€ç¼–ç ä¹±ç é—®é¢˜
"""

import feedparser
import requests
import json
import os
import re
import random
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin

# é…ç½®è·¯å¾„
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_FILE = os.path.join(BASE_DIR, 'data', 'data.json')  # æ³¨æ„å‰ç«¯è¯»å–çš„æ˜¯ data.json

# æµè§ˆå™¨ä¼ªè£…å¤´ï¼ˆå…³é”®ï¼æ²¡è¿™ä¸ªä¼šè¢«ç½‘ç«™æ‹’ç»ï¼‰
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
}

# RSS æºé…ç½®ï¼ˆåŒ¹é…ä½ å‰ç«¯çš„5ä¸ªåˆ†ç±»ï¼‰
RSS_SOURCES = {
    "shanghai": [  # ä¸Šæµ·æ–°é—»
        {"name": "ç•Œé¢æ–°é—»", "url": "https://a.jiemian.com/rss/"},
        {"name": "ä¸Šè§‚æ–°é—»", "url": "https://www.shobserver.com/rss.xml"},
        {"name": "æ¾æ¹ƒæ–°é—»", "url": "https://www.thepaper.cn/rss.xml"},  # å¯èƒ½ä¸ç¨³å®šï¼Œæœ‰å¤‡ç”¨
    ],
    "policy": [  # å›½å†…æ”¿ç­–
        {"name": "äººæ°‘ç½‘æ—¶æ”¿", "url": "http://rss.people.com.cn/rss/politics.xml"},
        {"name": "æ–°åç½‘æ—¶æ”¿", "url": "http://www.xinhuanet.com/rss/politics.xml"},
    ],
    "world": [  # å›½é™…æ–°é—»
        {"name": "BBCä¸­æ–‡", "url": "https://feeds.bbci.co.uk/zhongwen/simp/rss.xml"},
        {"name": "FTä¸­æ–‡ç½‘", "url": "https://www.ftchinese.com/rss/news"},
        {"name": "è”åˆæ—©æŠ¥", "url": "https://www.zaobao.com/rss.xml"},
    ],
    "tech": [  # AIå‰æ²¿/ç§‘æŠ€
        {"name": "Solidot", "url": "https://www.solidot.org/index.rss"},
        {"name": "Hacker News", "url": "https://news.ycombinator.com/rss"},
        {"name": "TechCrunchä¸­æ–‡", "url": "https://techcrunch.cn/rss/"},
    ],
    "us_stocks": [  # ç¾è‚¡ï¼ˆå…ˆæä¾›è´¢ç»æ–°é—»ï¼Œå¦‚éœ€è‚¡ä»·å¯åç»­æ·»åŠ ï¼‰
        {"name": "æ–°æµªè´¢ç»ç¾è‚¡", "url": "https://rss.sina.com.cn/finance/usstock.xml"},
        {"name": "åå°”è¡—æ—¥æŠ¥", "url": "https://cn.wsj.com/zh-hans/rss/markets.xml"},
    ]
}

def extract_image_from_entry(entry, source_url):
    """
    ä»RSSæ¡ç›®ä¸­æå–å›¾ç‰‡ï¼ˆå¤šç§ç­–ç•¥ï¼‰
    """
    # ç­–ç•¥1ï¼šæ£€æŸ¥ media:content (æœ€å¸¸è§)
    if 'media_content' in entry and entry.media_content:
        for media in entry.media_content:
            if media.get('medium') == 'image' or media.get('type', '').startswith('image'):
                return media.get('url', '')
    
    # ç­–ç•¥2ï¼šæ£€æŸ¥ enclosures
    if 'enclosures' in entry and entry.enclosures:
        for enc in entry.enclosures:
            if enc.get('type', '').startswith('image'):
                return enc.get('href', '')
    
    # ç­–ç•¥3ï¼šæ£€æŸ¥ summary/description ä¸­çš„ img æ ‡ç­¾
    summary = entry.get('summary', entry.get('description', ''))
    if summary:
        img_match = re.search(r'<img[^>]+src=["\'](https?://[^"\']+)["\']', summary)
        if img_match:
            return img_match.group(1)
    
    # ç­–ç•¥4ï¼šè®¿é—®åŸç½‘é¡µæŠ“å–ï¼ˆæ¯”è¾ƒæ…¢ï¼ŒåªæŠ“å‰å‡ æ¡æ—¶å¯ç”¨ï¼‰
    # ä¸ºäº†é€Ÿåº¦ï¼Œè¿™é‡Œæš‚ä¸å®ç°ï¼Œå¦‚éœ€å¯å¼€å¯
    return ""

def fetch_rss_feed(source_name, feed_url, limit=8):
    """
    æŠ“å–å•ä¸ªRSSæº
    """
    news_list = []
    
    try:
        print(f"ğŸ“¡ æ­£åœ¨æŠ“å–: {source_name}...")
        
        # ä¸‹è½½RSSå†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰
        for attempt in range(3):
            try:
                resp = requests.get(feed_url, headers=HEADERS, timeout=15)
                resp.encoding = resp.apparent_encoding  # è‡ªåŠ¨è¯†åˆ«ä¸­æ–‡ç¼–ç 
                break
            except Exception as e:
                if attempt == 2:
                    print(f"   âŒ {source_name} è¯·æ±‚å¤±è´¥: {e}")
                    return []
                time.sleep(1)
        
        # è§£æRSS
        feed = feedparser.parse(resp.text)
        
        for i, entry in enumerate(feed.entries[:limit]):
            try:
                # æå–å‘å¸ƒæ—¶é—´
                published = ""
                if hasattr(entry, 'published'):
                    published = entry.published
                elif hasattr(entry, 'updated'):
                    published = entry.updated
                else:
                    published = datetime.now().strftime("%Y-%m-%d %H:%M")
                
                # æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼ï¼ˆå‰ç«¯æœŸæœ› 2024-01-15 12:30 è¿™ç§æ ¼å¼ï¼‰
                try:
                    if 'T' in published:
                        # ISOæ ¼å¼ 2024-01-15T12:30:00Z
                        dt = datetime.fromisoformat(published.replace('Z', '+00:00'))
                        published = dt.strftime("%Y-%m-%d %H:%M")
                    else:
                        # å…¶ä»–æ ¼å¼å°è¯•è§£æ
                        dt = datetime.strptime(published[:16], "%Y-%m-%d %H:%M")
                        published = dt.strftime("%Y-%m-%d %H:%M")
                except:
                    pass  # ä¿æŒåŸæ ·
                
                # æå–æ‘˜è¦ï¼ˆå»é™¤HTMLæ ‡ç­¾ï¼‰
                summary = entry.get('summary', entry.get('description', ''))
                if summary:
                    # æ¸…ç†HTMLæ ‡ç­¾
                    summary = re.sub(r'<[^>]+>', '', summary)
                    # æ¸…ç†å¤šä½™ç©ºç™½
                    summary = re.sub(r'\s+', ' ', summary).strip()
                    # é™åˆ¶é•¿åº¦
                    if len(summary) > 200:
                        summary = summary[:200] + "..."
                else:
                    summary = "ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…..."
                
                # æå–å›¾ç‰‡
                image_url = extract_image_from_entry(entry, entry.get('link', ''))
                
                # å¦‚æœæ²¡å›¾ï¼Œä½¿ç”¨é»˜è®¤å ä½å›¾ï¼ˆå¯é€‰ï¼šä½ ä¹Ÿå¯ä»¥å‡†å¤‡å‡ å¼ é»˜è®¤å›¾è½®æ¢ï¼‰
                if not image_url:
                    # ä½¿ç”¨picsuméšæœºå›¾ä½œä¸ºå ä½ï¼Œæˆ–ç•™ç©ºè®©å‰ç«¯å¤„ç†
                    image_url = f"https://picsum.photos/seed/{hash(entry.title) % 1000}/400/300"
                
                news_item = {
                    "title": entry.get('title', 'æ— æ ‡é¢˜').strip(),
                    "link": entry.get('link', ''),
                    "summary": summary,
                    "source": source_name,
                    "time": published,
                    "image": image_url,
                    "category": ""  # ä¼šåœ¨å¤–å±‚å¡«å……
                }
                
                news_list.append(news_item)
                
            except Exception as e:
                print(f"   âš ï¸ è§£æå•æ¡å¤±è´¥: {e}")
                continue
        
        print(f"   âœ… æˆåŠŸè·å– {len(news_list)} æ¡")
        
    except Exception as e:
        print(f"   âŒ {source_name} å‡ºé”™: {e}")
    
    return news_list

def fetch_all_news():
    """
    æŠ“å–æ‰€æœ‰åˆ†ç±»æ–°é—»
    """
    all_data = {
        "shanghai": [],
        "policy": [],
        "world": [],
        "tech": [],
        "us_stocks": []
    }
    
    # æŠ“å–æ¯ä¸ªåˆ†ç±»
    for category, sources in RSS_SOURCES.items():
        print(f"\nğŸ“‚ åˆ†ç±»: {category}")
        category_news = []
        
        for source in sources:
            news = fetch_rss_feed(source["name"], source["url"], limit=6)
            # ç»™æ¯æ¡æ–°é—»æ‰“ä¸Šåˆ†ç±»æ ‡ç­¾
            for item in news:
                item["category"] = category
            category_news.extend(news)
            time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…è¢«å°
        
        # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ï¼‰
        seen_titles = set()
        unique_news = []
        for item in category_news:
            if item["title"] not in seen_titles:
                seen_titles.add(item["title"])
                unique_news.append(item)
        
        all_data[category] = unique_news
    
    # ç¡®ä¿ data ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(DATA_FILE), exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONï¼ˆå‰ç«¯ç›´æ¥è¯»å–è¿™ä¸ªæ–‡ä»¶ï¼‰
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {DATA_FILE}")
    print(f"ğŸ“Š æ€»è®¡: {sum(len(v) for v in all_data.values())} æ¡æ–°é—»")
    return all_data

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æŠ“å–æ–°é—»...")
    print("=" * 50)
    fetch_all_news()
    print("=" * 50)
    print("âœ… å®Œæˆï¼")
