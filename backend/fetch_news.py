import json
import os
import re
import time
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# ================= é…ç½®åŒºåŸŸ =================

# æ¨¡æ‹Ÿæµè§ˆå™¨èº«ä»½ï¼Œé˜²æ­¢è¢«æ‹¦æˆª
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# RSS æ•°æ®æºé…ç½® (æœ€ç¨³å®šçš„è·å–æ–¹å¼)
RSS_SOURCES = {
    # ğŸ™ï¸ ä¸Šæµ·æœ¬åœ°
    'shanghai': [
        'https://m.thepaper.cn/rss.jsp?nodeid=25635',  # æ¾æ¹ƒæ–°é—»-ä¸Šæµ·
        'https://www.shobserver.com/rss/index.html',    # ä¸Šè§‚æ–°é—»
    ],
    # ğŸŒ å›½é™…æ–°é—»
    'world': [
        'https://rss.huanqiu.com/hq/world.xml',         # ç¯çƒç½‘-å›½é™…
        'http://www.ftchinese.com/rss/news',            # FTä¸­æ–‡ç½‘
    ],
    # ğŸ¤– AIä¸ç§‘æŠ€
    'ai': [
        'https://www.36kr.com/feed',                    # 36æ°ª (ç§‘æŠ€åˆ›æŠ•)
        'https://www.ifanr.com/feed',                   # çˆ±èŒƒå„¿
    ],
    # ğŸ“ˆ ç¾è‚¡ä¸è´¢ç»
    'stocks': [
        'https://feed.wallstreetcn.com/feed/live',      # åå°”è¡—è§é—»
        'https://www.gelonghui.com/rss_feed.xml',       # æ ¼éš†æ±‡
    ],
    # ğŸ‡¨ğŸ‡³ æ”¿ç­–è§£è¯»
    'policy': [
        'http://www.news.cn/politics/news.xml',         # æ–°åç½‘-æ—¶æ”¿
        'https://m.thepaper.cn/rss.jsp?nodeid=25429',   # æ¾æ¹ƒæ–°é—»-æ—¶äº‹
    ]
}

# é»˜è®¤å ä½å›¾ï¼ˆå½“æ–°é—»æ²¡æœ‰å›¾ç‰‡æ—¶ä½¿ç”¨ï¼‰
DEFAULT_IMAGES = [
    "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=500&q=80",
    "https://images.unsplash.com/photo-1495020689067-958852a7765e?w=500&q=80",
    "https://images.unsplash.com/photo-1557683316-973673baf926?w=500&q=80",
    "https://images.unsplash.com/photo-1526304640156-00011457838e?w=500&q=80",
]

# ================= æ ¸å¿ƒåŠŸèƒ½ =================

def parse_rss_feed(url, category):
    """è§£æå•ä¸ª RSS æº"""
    print(f"æ­£åœ¨æŠ“å– [{category}]: {url}")
    news_items = []
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.encoding = 'utf-8' # å¼ºåˆ¶utf-8
        
        # ç®€å•çš„XMLè§£æ
        try:
            root = ET.fromstring(response.content)
        except:
            # å°è¯•ä¿®å¤ä¸€äº›å¸¸è§çš„XMLæ ¼å¼é”™è¯¯
            content = response.text.replace('&', '&amp;')
            root = ET.fromstring(content)

        # éå†æ–°é—»æ¡ç›® (é€‚é… RSS 2.0 å’Œ Atom)
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        for item in items[:15]: # æ¯ä¸ªæºåªå–å‰15æ¡
            try:
                # è·å–æ ‡é¢˜
                title = item.find('title').text if item.find('title') is not None else "æ— æ ‡é¢˜"
                
                # è·å–é“¾æ¥
                link = item.find('link').text if item.find('link') is not None else ""
                
                # è·å–æè¿°/æ‘˜è¦
                description = ""
                desc_tag = item.find('description') or item.find('content:encoded')
                if desc_tag is not None and desc_tag.text:
                    # å»é™¤HTMLæ ‡ç­¾ï¼Œåªç•™çº¯æ–‡æœ¬
                    description = re.sub(r'<[^>]+>', '', desc_tag.text)[:100] + "..."

                # è·å–æ—¶é—´
                pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                # ç®€å•æ ¼å¼åŒ–æ—¶é—´
                if pub_date:
                    try:
                        # å°è¯•å°† UTC æ ¼å¼è½¬ä¸ºç®€å•æ ¼å¼
                        pub_date = pub_date[:16] 
                    except:
                        pass
                
                # å°è¯•æå–å›¾ç‰‡ (ä»æè¿°ä¸­æ‰¾ img æ ‡ç­¾)
                image = ""
                if desc_tag is not None and desc_tag.text:
                    img_match = re.search(r'src="(http[^"]+\.(jpg|png|jpeg))"', desc_tag.text)
                    if img_match:
                        image = img_match.group(1)
                
                # å¦‚æœæ²¡æ‰¾åˆ°å›¾ç‰‡ï¼Œç»™ä¸€ä¸ªéšæœºå ä½å›¾ï¼Œæˆ–è€…æ ¹æ®åˆ†ç±»ç»™ç‰¹å®šå›¾
                if not image:
                    # è¿™é‡Œä½ å¯ä»¥åŠ é€»è¾‘ï¼Œç°åœ¨å…ˆç•™ç©ºï¼Œå‰ç«¯å¯ä»¥ç”¨CSSç”Ÿæˆæ¸å˜
                    image = "" 

                # æ¥æºè¯†åˆ«
                source_name = "ç½‘ç»œæ–°é—»"
                if "thepaper" in url: source_name = "æ¾æ¹ƒæ–°é—»"
                elif "36kr" in url: source_name = "36æ°ª"
                elif "huanqiu" in url: source_name = "ç¯çƒç½‘"
                elif "wallstreet" in url: source_name = "åå°”è¡—è§é—»"
                elif "news.cn" in url: source_name = "æ–°åç½‘"

                news_items.append({
                    "title": title.strip(),
                    "link": link,
                    "time": pub_date,
                    "source": source_name,
                    "image": image,
                    "summary": description,
                    "category": category
                })
            except Exception as e:
                continue
                
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ {url}: {e}")
        
    return news_items

def fetch_all_news():
    """ä¸»ç¨‹åºï¼šå¹¶è¡ŒæŠ“å–æ‰€æœ‰æ–°é—»"""
    all_news = {
        "shanghai": [],
        "world": [],
        "ai": [],
        "stocks": [],
        "policy": []
    }
    
    tasks = []
    # ä½¿ç”¨çº¿ç¨‹æ± åŠ å¿«é€Ÿåº¦
    with ThreadPoolExecutor(max_workers=5) as executor:
        for category, urls in RSS_SOURCES.items():
            for url in urls:
                tasks.append(executor.submit(parse_rss_feed, url, category))
    
    # æ”¶é›†ç»“æœ
    for future in tasks:
        items = future.result()
        if items:
            # æ‹¿åˆ°ç»“æœåï¼Œæ”¾å…¥å¯¹åº”çš„åˆ†ç±»
            cat = items[0]['category']
            all_news[cat].extend(items)

    # ä¿å­˜åˆ°ä¸Šä¸€çº§ç›®å½•çš„ data.json
    output_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data.json')
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… æ›´æ–°å®Œæˆï¼å…±è·å–æ–°é—»ï¼š{sum(len(v) for v in all_news.values())} æ¡")

if __name__ == "__main__":
    fetch_all_news()
