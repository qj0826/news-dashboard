import json
import os
import re
import time
import random
import requests
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor

# ================= 1. æ·±åº¦å®šåˆ¶æ–°é—»æº =================

# æ¨¡æ‹Ÿæµè§ˆå™¨èº«ä»½ï¼Œé˜²æ­¢è¢«åçˆ¬è™«æ‹¦æˆª
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

RSS_SOURCES = {
    # ğŸ™ï¸ ä¸Šæµ·æœ¬åœ° (èšç„¦æ°‘ç”Ÿã€æœ¬åœ°æ”¿ç­–)
    'shanghai': [
        'https://m.thepaper.cn/rss.jsp?nodeid=25635',  # æ¾æ¹ƒæ–°é—»-ä¸Šæµ· (æœ€æƒå¨)
        'https://www.shobserver.com/rss/index.html',    # ä¸Šè§‚æ–°é—» (å®˜æ–¹å…šæŠ¥)
    ],
    
    # ğŸ¤– ç§‘æŠ€å‰æ²¿ (AIã€ç¡¬ç§‘æŠ€)
    'tech': [
        'https://www.36kr.com/feed',                    # 36æ°ª (åˆ›æŠ•ç¬¬ä¸€çº¿)
        'https://www.ifanr.com/feed',                   # çˆ±èŒƒå„¿ (äº§å“ä¸è§‚ç‚¹)
        'https://rss.cnbeta.com.tw/rss',                # cnBeta (ç¡¬æ ¸ITæ–°é—»)
    ],
    
    # ğŸ“ˆ ç¾è‚¡ä¸æŒä»“ (æ›¿ä»£ Xï¼šä½¿ç”¨é«˜é¢‘å¿«è®¯æº)
    # è¿™é‡Œé€‰ç”¨äº†å›½å†…é€Ÿåº¦æœ€å¿«çš„ç¾è‚¡èµ„è®¯ï¼Œä¸“é—¨è¦†ç›–æœºæ„æŒä»“ã€ç›˜å‰å¼‚åŠ¨
    'us_stocks': [
        'https://feed.wallstreetcn.com/feed/live',      # åå°”è¡—è§é—»-å®æ—¶å¿«è®¯ (æœ€æ¥è¿‘ Twitter ä½“éªŒ)
        'https://www.gelonghui.com/rss_feed.xml',       # æ ¼éš†æ±‡-æ¸¯ç¾è‚¡ (ä¸“ä¸šçš„æŠ•èµ„ç¤¾åŒº)
    ],
    
    # ğŸ‡¨ğŸ‡³ å›½å†…æ”¿ç­– (å®è§‚ç»æµã€é¡¶å±‚è®¾è®¡)
    'policy': [
        'http://www.news.cn/politics/news.xml',         # æ–°åç½‘-æ—¶æ”¿ (æœ€å®˜æ–¹)
        'https://m.thepaper.cn/rss.jsp?nodeid=25429',   # æ¾æ¹ƒæ–°é—»-æ—¶äº‹ (è§£è¯»è¾ƒå¤š)
        'http://www.chinanews.com/rss/gn.xml',          # ä¸­å›½æ–°é—»ç½‘-å›½å†…
    ]
}

# ================= 2. æ™ºèƒ½å›¾åº“ (è‡ªåŠ¨ç¾åŒ–) =================
# å½“æ–°é—»æ²¡æœ‰é…å›¾æ—¶ï¼Œæ ¹æ®æ¿å—è‡ªåŠ¨åŒ¹é…ä¸€å¼ é«˜å¤§ä¸Šçš„èƒŒæ™¯å›¾
DEFAULT_IMAGES = {
    'shanghai': [
        "https://images.unsplash.com/photo-1474181487882-5abf3f0ba6c2?w=600&q=80", # å¤–æ»©
        "https://images.unsplash.com/photo-1548919973-5cef591cdbc9?w=600&q=80", # ä¸Šæµ·é«˜æ¥¼
    ],
    'tech': [
        "https://images.unsplash.com/photo-1518770660439-4636190af475?w=600&q=80", # èŠ¯ç‰‡
        "https://images.unsplash.com/photo-1531297420492-6029d146dc29?w=600&q=80", # ä»£ç 
    ],
    'us_stocks': [
        "https://images.unsplash.com/photo-1611974765270-ca1258822981?w=600&q=80", # è‚¡å¸‚Kçº¿
        "https://images.unsplash.com/photo-1590283603385-17ffb3a7f29f?w=600&q=80", # åå°”è¡—ç‰›
    ],
    'policy': [
        "https://images.unsplash.com/photo-1532375810709-75b1da00537c?w=600&q=80", # ä¸¥è‚ƒå»ºç­‘
        "https://images.unsplash.com/photo-1529101091760-6149d4c46b27?w=600&q=80", # ç¬”ä¸çº¸
    ]
}

# ================= 3. æ ¸å¿ƒæŠ“å–é€»è¾‘ =================

def parse_rss_feed(url, category):
    print(f"æ­£åœ¨æŠ“å– [{category}]: {url}")
    news_items = []
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.encoding = 'utf-8'
        
        # å¤„ç† XML è§£æ
        try:
            root = ET.fromstring(response.content)
        except:
            # å®¹é”™å¤„ç†ï¼šæ›¿æ¢ç‰¹æ®Šå­—ç¬¦
            content = response.text.replace('&', '&amp;')
            root = ET.fromstring(content)

        # å…¼å®¹ RSS å’Œ Atom ä¸¤ç§æ ¼å¼
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        for item in items[:20]: # æ¯ä¸ªæºå¤šæŠ“ä¸€ç‚¹ï¼Œå–å‰20æ¡
            try:
                # 1. æå–æ ‡é¢˜
                title = item.find('title').text if item.find('title') is not None else "æœ€æ–°èµ„è®¯"
                
                # 2. æå–é“¾æ¥
                link = item.find('link').text if item.find('link') is not None else ""
                
                # 3. æå–ç®€ä»‹
                description = ""
                desc_tag = item.find('description') or item.find('content:encoded')
                if desc_tag is not None and desc_tag.text:
                    # å»é™¤ HTML æ ‡ç­¾ï¼Œåªç•™çº¯æ–‡å­—
                    clean_text = re.sub(r'<[^>]+>', '', desc_tag.text)
                    description = clean_text[:120] + "..." if len(clean_text) > 120 else clean_text

                # 4. æå–æ—¶é—´
                pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                if pub_date:
                    # ç®€å•æˆªå–æ—¶é—´å­—ç¬¦ä¸²
                    pub_date = pub_date[:16]

                # 5. æ™ºèƒ½æå–å›¾ç‰‡
                image = ""
                if desc_tag is not None and desc_tag.text:
                    # æ­£åˆ™åŒ¹é… src="xxx.jpg"
                    img_match = re.search(r'src="(http[^"]+\.(jpg|png|jpeg|webp))"', desc_tag.text)
                    if img_match:
                        image = img_match.group(1)
                
                # å¦‚æœæ²¡å›¾ï¼Œä»å¯¹åº”åˆ†ç±»çš„å›¾åº“é‡ŒéšæœºæŠ½ä¸€å¼ 
                if not image:
                    image = random.choice(DEFAULT_IMAGES.get(category, DEFAULT_IMAGES['tech']))

                # 6. æ¥æºæ ‡è®°
                source_name = "ç½‘ç»œ"
                if "thepaper" in url: source_name = "æ¾æ¹ƒæ–°é—»"
                elif "shobserver" in url: source_name = "ä¸Šè§‚æ–°é—»"
                elif "36kr" in url: source_name = "36æ°ª"
                elif "wallstreet" in url: source_name = "åå°”è¡—è§é—»"
                elif "gelonghui" in url: source_name = "æ ¼éš†æ±‡"
                elif "news.cn" in url: source_name = "æ–°åç½‘"
                elif "ifanr" in url: source_name = "çˆ±èŒƒå„¿"

                # å­˜å…¥åˆ—è¡¨
                news_items.append({
                    "title": title.strip(),
                    "link": link,
                    "time": pub_date,
                    "source": source_name,
                    "image": image,
                    "summary": description,
                    "category": category
                })
            except Exception:
                continue
                
    except Exception as e:
        print(f"âŒ {url} æŠ“å–å‡ºé”™: {e}")
        
    return news_items

def fetch_all_news():
    """ä¸»ç¨‹åº"""
    all_news = {
        "shanghai": [],
        "tech": [],
        "us_stocks": [],
        "policy": []
    }
    
    tasks = []
    # å¼€å¯5ä¸ªçº¿ç¨‹å¹¶è¡ŒæŠ“å–ï¼Œé€Ÿåº¦æ›´å¿«
    with ThreadPoolExecutor(max_workers=5) as executor:
        for category, urls in RSS_SOURCES.items():
            for url in urls:
                tasks.append(executor.submit(parse_rss_feed, url, category))
    
    # æ”¶é›†ç»“æœ
    for future in tasks:
        items = future.result()
        if items:
            cat = items[0]['category']
            all_news[cat].extend(items)

    # è·¯å¾„å¤„ç†ï¼šä¿å­˜åˆ°ä¸Šçº§ç›®å½•çš„ data.json
    output_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data.json')
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… æ›´æ–°å®Œæˆï¼æ€»è®¡è·å–æ–°é—»: {sum(len(v) for v in all_news.values())} æ¡")

if __name__ == "__main__":
    fetch_all_news()
