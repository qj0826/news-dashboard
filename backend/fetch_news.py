import json
import os
import re
import time
import random  # æ–°å¢ï¼šç”¨äºéšæœºæŠ½å›¾
import requests
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor

# ================= é…ç½®åŒºåŸŸ =================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

RSS_SOURCES = {
    'shanghai': [
        'https://m.thepaper.cn/rss.jsp?nodeid=25635',
        'https://www.shobserver.com/rss/index.html',
    ],
    'world': [
        'https://rss.huanqiu.com/hq/world.xml',
        'http://www.ftchinese.com/rss/news',
    ],
    'ai': [
        'https://www.36kr.com/feed',
        'https://www.ifanr.com/feed',
    ],
    'stocks': [
        'https://feed.wallstreetcn.com/feed/live',
        'https://www.gelonghui.com/rss_feed.xml',
    ],
    'policy': [
        'http://www.news.cn/politics/news.xml',
        'https://m.thepaper.cn/rss.jsp?nodeid=25429',
    ]
}

# ğŸ–¼ï¸ é»˜è®¤å›¾åº“ï¼ˆå½“æ–°é—»æ²¡å›¾æ—¶ï¼Œéšæœºä»è¿™é‡Œé€‰ä¸€å¼ ï¼Œçœ‹èµ·æ¥æ›´ä¸°å¯Œï¼‰
DEFAULT_IMAGES = [
    "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=600&q=80", # æ–°é—»çº¸
    "https://images.unsplash.com/photo-1495020689067-958852a7765e?w=600&q=80", # æŠ¥çº¸å †
    "https://images.unsplash.com/photo-1557683316-973673baf926?w=600&q=80",   # æŠ½è±¡æ¸å˜
    "https://images.unsplash.com/photo-1526304640156-00011457838e?w=600&q=80", # ç§‘æŠ€æ„Ÿ
    "https://images.unsplash.com/photo-1503694987629-9479c8d9e918?w=600&q=80", # åŠå…¬æ¡Œ
    "https://images.unsplash.com/photo-1486406146926-c627a92ad1ab?w=600&q=80", # åŸå¸‚å»ºç­‘
    "https://images.unsplash.com/photo-1518770660439-4636190af475?w=600&q=80", # èŠ¯ç‰‡ç§‘æŠ€
    "https://images.unsplash.com/photo-1611974765270-ca1258822981?w=600&q=80", # è‚¡å¸‚Kçº¿
]

def parse_rss_feed(url, category):
    print(f"æ­£åœ¨æŠ“å– [{category}]: {url}")
    news_items = []
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.encoding = 'utf-8'
        
        try:
            root = ET.fromstring(response.content)
        except:
            content = response.text.replace('&', '&amp;')
            root = ET.fromstring(content)

        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        for item in items[:15]:
            try:
                title = item.find('title').text if item.find('title') is not None else "æ— æ ‡é¢˜"
                link = item.find('link').text if item.find('link') is not None else ""
                
                description = ""
                desc_tag = item.find('description') or item.find('content:encoded')
                if desc_tag is not None and desc_tag.text:
                    description = re.sub(r'<[^>]+>', '', desc_tag.text)[:100] + "..."

                pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                if pub_date:
                    try:
                        pub_date = pub_date[:16] 
                    except:
                        pass
                
                # 1. ä¼˜å…ˆå°è¯•ä»å†…å®¹é‡Œæ‰¾å›¾ç‰‡
                image = ""
                if desc_tag is not None and desc_tag.text:
                    img_match = re.search(r'src="(http[^"]+\.(jpg|png|jpeg))"', desc_tag.text)
                    if img_match:
                        image = img_match.group(1)
                
                # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œéšæœºåˆ†é…ä¸€å¼ å¥½çœ‹çš„å›¾
                if not image:
                    image = random.choice(DEFAULT_IMAGES)

                source_name = "ç½‘ç»œæ–°é—»"
                if "thepaper" in url: source_name = "æ¾æ¹ƒæ–°é—»"
                elif "36kr" in url: source_name = "36æ°ª"
                elif "huanqiu" in url: source_name = "ç¯çƒç½‘"
                elif "wallstreet" in url: source_name = "åå°”è¡—è§é—»"
                elif "news.cn" in url: source_name = "æ–°åç½‘"
                elif "shobserver" in url: source_name = "ä¸Šè§‚æ–°é—»"
                elif "ftchinese" in url: source_name = "FTä¸­æ–‡"

                news_items.append({
                    "title": title.strip(),
                    "link": link,
                    "time": pub_date,
                    "source": source_name,
                    "image": image,
                    "summary": description,
                    "category": category
                })
            except Exception:
                continue
                
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ {url}: {e}")
        
    return news_items

def fetch_all_news():
    all_news = {"shanghai":[], "world":[], "ai":[], "stocks":[], "policy":[]}
    
    tasks = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        for category, urls in RSS_SOURCES.items():
            for url in urls:
                tasks.append(executor.submit(parse_rss_feed, url, category))
    
    for future in tasks:
        items = future.result()
        if items:
            cat = items[0]['category']
            all_news[cat].extend(items)

    output_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data.json')
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… æ›´æ–°å®Œæˆï¼å…±è·å–æ–°é—»ï¼š{sum(len(v) for v in all_news.values())} æ¡")

if __name__ == "__main__":
    fetch_all_news()
