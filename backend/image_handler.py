#!/usr/bin/env python3
"""
æ–°é—»å°é¢å›¾ç‰‡å¤„ç†æ¨¡å—
- æŠ“å–çœŸå® og:image
- æ— å›¾æ—¶ç”¨ AI ç”Ÿæˆ
"""

import requests
import re
import urllib.parse
from pathlib import Path
import hashlib

# ä»£ç†é…ç½®
PROXY = {'http': 'http://127.0.0.1:1082', 'https': 'http://127.0.0.1:1082'}

# Pollinations.ai API Key
POLLINATIONS_API_KEY = 'pk_AwoOxyA1F7BjqSCq'

# åˆ†ç±»å¯¹åº”çš„ AI ç”Ÿæˆæç¤ºè¯
CATEGORY_PROMPTS = {
    'shanghai': 'ä¸Šæµ·åŸå¸‚é£å…‰ï¼Œç°ä»£å»ºç­‘ï¼Œæš–è‰²è°ƒï¼Œæ–°é—»é…å›¾é£æ ¼ï¼Œç®€æ´å¤§æ°”',
    'world': 'å›½é™…æ–°é—»ï¼Œåœ°çƒï¼Œå…¨çƒè§†é‡ï¼Œè“è‰²è°ƒï¼Œä¸“ä¸šæ–°é—»é…å›¾',
    'ai': 'äººå·¥æ™ºèƒ½ï¼Œç§‘æŠ€æ„Ÿï¼Œè“è‰²ç´«è‰²æ¸å˜ï¼Œæœªæ¥æ„Ÿï¼ŒAIæ–°é—»é…å›¾',
    'stocks': 'é‡‘èè‚¡ç¥¨ï¼Œä¸Šå‡æ›²çº¿ï¼Œé‡‘è‰²ç»¿è‰²ï¼Œå•†åŠ¡ä¸“ä¸šé£æ ¼',
    'policy': 'ä¸­å›½æ”¿åºœå»ºç­‘ï¼Œçº¢è‰²å…ƒç´ ï¼Œåº„é‡æ­£å¼ï¼Œæ”¿ç­–æ–°é—»é…å›¾'
}

def fetch_og_image(url):
    """ä»ç½‘é¡µæŠ“å– og:image"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # é™åˆ¶é¡µé¢å¤§å°ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY, stream=True)
        response.raise_for_status()
        
        # åªè¯»å–å‰ 100KBï¼ˆè¶³å¤Ÿæ‰¾åˆ° meta æ ‡ç­¾ï¼‰
        content = b''
        for chunk in response.iter_content(chunk_size=1024):
            content += chunk
            if len(content) > 100000:
                break
        
        html = content.decode('utf-8', errors='ignore')
        
        # æŸ¥æ‰¾ og:image
        patterns = [
            r'<meta[^>]*property="og:image"[^>]*content="([^"]+)"',
            r'<meta[^>]*content="([^"]+)"[^>]*property="og:image"',
            r'<meta[^>]*name="twitter:image"[^>]*content="([^"]+)"',
            r'<meta[^>]*property="og:image:url"[^>]*content="([^"]+)"',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html, re.IGNORECASE)
            if match:
                image_url = match.group(1).strip()
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif image_url.startswith('/'):
                    parsed = urllib.parse.urlparse(url)
                    image_url = f"{parsed.scheme}://{parsed.netloc}{image_url}"
                
                # éªŒè¯å›¾ç‰‡ URL æ˜¯å¦å¯è®¿é—®
                if validate_image_url(image_url):
                    return image_url
        
        return None
    except Exception as e:
        return None

def validate_image_url(url):
    """éªŒè¯å›¾ç‰‡ URL æ˜¯å¦æœ‰æ•ˆ"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        response = requests.head(url, headers=headers, timeout=5, proxies=PROXY, allow_redirects=True)
        
        if response.status_code == 200:
            content_type = response.headers.get('content-type', '').lower()
            return 'image' in content_type
        return False
    except:
        return False

def generate_ai_image(title, category='general'):
    """ä½¿ç”¨ Pollinations.ai ç”Ÿæˆå°é¢å›¾ç‰‡ï¼ˆä½¿ç”¨ API Key è·å¾—æ›´å¿«æ›´ç¨³å®šçš„æœåŠ¡ï¼‰"""
    try:
        # æ„å»ºæç¤ºè¯
        base_prompt = CATEGORY_PROMPTS.get(category, 'æ–°é—»é…å›¾ï¼Œä¸“ä¸šæ‘„å½±é£æ ¼ï¼Œé«˜è´¨é‡')

        # ç®€åŒ–æ ‡é¢˜ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_title = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', title)[:30]

        # ç»„åˆæç¤ºè¯
        prompt = f"{base_prompt}ï¼Œä¸»é¢˜ï¼š{clean_title}ï¼Œä¸“ä¸šæ‘„å½±ï¼Œé«˜æ¸…"

        # ä½¿ç”¨ seed ç¡®ä¿ç›¸åŒæ ‡é¢˜ç”Ÿæˆç›¸åŒå›¾ç‰‡ï¼ˆå¯ç¼“å­˜ï¼‰
        seed = int(hashlib.md5(title.encode()).hexdigest(), 16) % 10000

        # Pollinations.ai APIï¼ˆå¸¦ key è·å¾—æ›´å¿«ç”Ÿæˆé€Ÿåº¦ï¼‰
        encoded_prompt = urllib.parse.quote(prompt)
        image_url = f"https://image.pollinations.ai/prompt/{encoded_prompt}?width=600&height=750&seed={seed}&nologo=true&token={POLLINATIONS_API_KEY}"

        return image_url
    except Exception as e:
        return None

def get_news_image(title, url, category='general', prefer_real=False):
    """
    è·å–æ–°é—»å°é¢å›¾ç‰‡
    
    Args:
        title: æ–°é—»æ ‡é¢˜
        url: æ–°é—»é“¾æ¥
        category: åˆ†ç±»
        prefer_real: ä¼˜å…ˆä½¿ç”¨çœŸå®å›¾ç‰‡ï¼ˆé»˜è®¤Falseï¼Œå› ä¸ºæŠ“å–è¾ƒæ…¢ï¼‰
    
    Returns:
        å›¾ç‰‡ URL æˆ– None
    """
    # ç›´æ¥ä½¿ç”¨ AI ç”Ÿæˆï¼Œé€Ÿåº¦æ›´å¿«
    image_url = generate_ai_image(title, category)
    if image_url:
        return {'url': image_url, 'type': 'ai'}
    
    return None

if __name__ == '__main__':
    # æµ‹è¯•
    test_cases = [
        ('ä¸Šæµ·å‘å¸ƒæ–°ä¸€è½®ä¼˜åŒ–è¥å•†ç¯å¢ƒæ–¹æ¡ˆ', 'https://www.shanghai.gov.cn', 'shanghai'),
        ('SpaceX æ˜Ÿèˆ°æœ€æ–°å‘å°„', 'https://www.spacex.com', 'world'),
        ('OpenAI å‘å¸ƒ GPT-5', 'https://openai.com', 'ai'),
    ]
    
    for title, url, cat in test_cases:
        print(f"\nğŸ“ {title}")
        result = get_news_image(title, url, cat)
        if result:
            print(f"   {'ğŸ“·' if result['type']=='real' else 'ğŸ¨'} {result['type'].upper()}: {result['url'][:80]}...")
        else:
            print("   âŒ æ— å›¾ç‰‡")
