#!/usr/bin/env python3
"""
æ–°é—»èšåˆæŠ“å–è„šæœ¬ - å®æ—¶ç‰ˆ
æ¯5åˆ†é’Ÿæ›´æ–°ï¼Œæ”¯æŒå…³é”®è¯æ¨é€
"""

import json
import requests
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import html
import urllib.parse
import sys

# å¯¼å…¥å›¾ç‰‡å¤„ç†æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from image_handler import get_news_image

DATA_DIR = Path(__file__).parent.parent / "data"
DATA_DIR.mkdir(exist_ok=True)

# ä»£ç†é…ç½®
PROXY = {'http': 'http://127.0.0.1:1082', 'https': 'http://127.0.0.1:1082'}

def translate_text(text, target_lang='zh-CN'):
    """ä½¿ç”¨ Google Translate å…è´¹ API ç¿»è¯‘"""
    try:
        if not text or len(text.strip()) == 0:
            return text
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return text  # å·²ç»æœ‰ä¸­æ–‡ï¼Œä¸ç¿»è¯‘
        
        # é™åˆ¶é•¿åº¦ï¼Œé¿å…APIé™åˆ¶
        text_to_translate = text[:500]
        
        # Google Translate å…è´¹ API
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl={target_lang}&dt=t&q={urllib.parse.quote(text_to_translate)}"
        
        response = requests.get(url, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            # è§£æè¿”å›ç»“æœ
            translated_parts = []
            for item in data[0]:
                if item[0]:
                    translated_parts.append(item[0])
            return ''.join(translated_parts)
        
        return text  # ç¿»è¯‘å¤±è´¥è¿”å›åŸæ–‡
    except Exception as e:
        return text  # å‡ºé”™è¿”å›åŸæ–‡

def format_time(published):
    """æ ¼å¼åŒ–æ—¶é—´"""
    if not published:
        return ""
    
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S GMT",
        "%Y-%m-%dT%H:%M:%S",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%d %H:%M:%S",
    ]
    
    for fmt in formats:
        try:
            clean_pub = published.strip().replace('+0800', '+08:00')
            dt = datetime.strptime(clean_pub[:26], fmt)
            return dt.strftime("%m-%d %H:%M")
        except:
            continue
    
    return published[:16] if len(published) > 16 else published

def is_recent(published_parsed):
    """åˆ¤æ–­æ˜¯å¦æ˜¯24å°æ—¶å†…çš„æ–°é—»"""
    if not published_parsed:
        return False
    try:
        from time import mktime
        pub_timestamp = mktime(published_parsed)
        pub_time = datetime.fromtimestamp(pub_timestamp)
        return datetime.now() - pub_time < timedelta(hours=24)
    except:
        return False

def fetch_reddit_worldnews():
    """æŠ“å– Reddit r/worldnews - æœ€å¿«"""
    items = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        url = "https://www.reddit.com/r/worldnews/new.json?limit=10"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            for post in data['data']['children']:
                post_data = post['data']
                title = html.unescape(post_data['title'])
                translated_title = translate_text(title)
                
                items.append({
                    "title": translated_title,
                    "link": "https://reddit.com" + post_data['permalink'],
                    "summary": f"â¬†ï¸ {post_data.get('score', 0)} | ğŸ’¬ {post_data.get('num_comments', 0)}",
                    "source": "Reddit å›½é™…æ–°é—»",
                    "time": datetime.fromtimestamp(post_data['created']).strftime("%m-%d %H:%M"),
                    "isNew": True
                })
        
        print(f"  âœ“ Reddit: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— Reddit: {str(e)[:50]}")
    
    return items

def is_shanghai_relevant(title, summary=""):
    """åˆ¤æ–­æ˜¯å¦ä¸å˜‰å®š/èŠ‚æ°”/ç¤¾åŒºç›¸å…³"""
    text = (title + summary).lower()
    
    # å˜‰å®šç›¸å…³
    jiading_keywords = ['å˜‰å®š', 'å—ç¿”', 'æ±Ÿæ¡¥', 'å®‰äº­', 'é©¬é™†', 'å¤–å†ˆ', 'å¾è¡Œ', 'åäº­', 'èŠå›­', 'æ–°æˆè·¯', 'çœŸæ–°', 'å˜‰å®šæ–°åŸ', 'å˜‰å®šå·¥ä¸šåŒº', 'å·æ¡¥', 'æ³•åå¡”']
    # èŠ‚æ°”æ—¶ä»¤
    season_keywords = ['ç«‹æ˜¥', 'é›¨æ°´', 'æƒŠè›°', 'æ˜¥åˆ†', 'æ¸…æ˜', 'è°·é›¨', 'ç«‹å¤', 'å°æ»¡', 'èŠ’ç§', 'å¤è‡³', 'å°æš‘', 'å¤§æš‘',
                       'ç«‹ç§‹', 'å¤„æš‘', 'ç™½éœ²', 'ç§‹åˆ†', 'å¯’éœ²', 'éœœé™', 'ç«‹å†¬', 'å°é›ª', 'å¤§é›ª', 'å†¬è‡³', 'å°å¯’', 'å¤§å¯’']
    # ç¤¾åŒºæ°‘ç”Ÿ
    community_keywords = ['ç¤¾åŒº', 'è¡—é“', 'å±…å§”ä¼š', 'ä¸šå§”ä¼š', 'ç‰©ä¸š', 'é‚»é‡Œ', 'ä¾¿æ°‘', 'ä¸ºè€', 'å…»è€', 'æ‰˜è‚²', 'èœåœº', 'æ—§æ”¹', 'åŠ è£…ç”µæ¢¯', 'é•¿æŠ¤é™©', 'åŒ»ä¿']
    
    has_jiading = any(kw in text for kw in jiading_keywords)
    has_season = any(kw in text for kw in season_keywords)
    has_community = any(kw in text for kw in community_keywords)
    
    return {
        'jiading': has_jiading,
        'season': has_season,
        'community': has_community,
        'score': int(has_jiading) * 3 + int(has_season) * 2 + int(has_community) * 1
    }

def fetch_shanghai_news():
    """æŠ“å–ä¸Šæµ·æ–°é—» - å¤šæºèšåˆ"""
    items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
    
    # 1. æ¾æ¹ƒæ–°é—» RSS
    try:
        url = "https://feedx.net/rss/thepaper.xml"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            
            for entry in feed.entries:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                # æ„å»ºæ ‡ç­¾
                tags = []
                if relevance['jiading']: tags.append('ğŸ ')
                if relevance['season']: tags.append('ğŸŒ¸')
                if relevance['community']: tags.append('ğŸ‘¥')
                
                # ä¼˜å…ˆæ·»åŠ ç›¸å…³æ–°é—»
                if relevance['score'] > 0 or len(items) < 20:
                    items.append({
                        "title": f"{' '.join(tags)} {title}" if tags else title,
                        "link": entry.get("link", ""),
                        "summary": f"æ¾æ¹ƒæ–°é—» Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ¾æ¹ƒæ–°é—» Â· ä¸Šæµ·",
                        "source": "æ¾æ¹ƒæ–°é—»",
                        "time": format_time(entry.get("published", "")),
                        "isNew": is_recent(entry.get("published_parsed")),
                        "score": relevance['score']
                    })
            
            print(f"  âœ“ æ¾æ¹ƒæ–°é—»: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— æ¾æ¹ƒæ–°é—»: {str(e)[:50]}")
    
    # 2. æ‰‹åŠ¨ç»´æŠ¤ - å˜‰å®šç²¾é€‰æ–°é—»
    print("\nğŸ  å˜‰å®šç²¾é€‰")
    jiading_manual = [
        {
            "title": "ğŸ  å˜‰å®šæ–°åŸå»ºè®¾æé€Ÿï¼Œå¤šä¸ªé‡å¤§é¡¹ç›®é›†ä¸­å¼€å·¥",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å˜‰å®šåŒºæ¨åŠ¨æ–°åŸå»ºè®¾ï¼Œèšç„¦ç§‘æŠ€åˆ›æ–°",
            "source": "å˜‰å®šå‘å¸ƒ",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 3
        },
        {
            "title": "ğŸ‘¥ å—ç¿”é•‡åŠ è£…ç”µæ¢¯å·¥ç¨‹åˆæœ‰æ–°è¿›å±•ï¼Œå¤šä¸ªå°åŒºå®Œæˆç­¾çº¦",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å—ç¿”é•‡æ¨è¿›è€æ—§å°åŒºåŠ è£…ç”µæ¢¯",
            "source": "å—ç¿”é•‡",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 3
        },
        {
            "title": "ğŸŒ¸ å·æ¡¥è€è¡—æ˜¥èŠ‚æ°‘ä¿—æ´»åŠ¨å®‰æ’å‡ºç‚‰",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å·æ¡¥è€è¡—æ˜¥èŠ‚æœŸé—´æ°‘ä¿—æ–‡åŒ–æ´»åŠ¨",
            "source": "å˜‰å®šæ–‡æ—…",
            "time": "02-01",
            "isNew": False,
            "score": 2
        },
        {
            "title": "ğŸ‘¥ æ±Ÿæ¡¥é•‡æ¨è¿›'15åˆ†é’Ÿç¤¾åŒºç”Ÿæ´»åœˆ'å»ºè®¾",
            "link": "https://www.jiading.gov.cn/",
            "summary": "æ±Ÿæ¡¥é•‡ä¾¿æ°‘æœåŠ¡è®¾æ–½å‡çº§",
            "source": "æ±Ÿæ¡¥é•‡",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 2
        },
    ]
    items.extend(jiading_manual)
    print(f"  âœ“ å˜‰å®šç²¾é€‰: {len(jiading_manual)} æ¡")
    
    # 3. æ–°æµªä¸Šæµ·æ–°é—»
    print("\nğŸ“° æ–°æµªä¸Šæµ·")
    try:
        url = "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2515&k=&num=20&r=0.123"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('result') and data['result'].get('data'):
                news_list = data['result']['data']
                for item in news_list:
                    title = item.get('title', '').strip()
                    url = item.get('url', '')
                    time_str = item.get('time', '')
                    
                    # æ£€æŸ¥ç›¸å…³æ€§
                    relevance = is_shanghai_relevant(title)
                    tags = []
                    if relevance['jiading']: tags.append('ğŸ ')
                    if relevance['season']: tags.append('ğŸŒ¸')
                    if relevance['community']: tags.append('ğŸ‘¥')
                    
                    items.append({
                        "title": f"{' '.join(tags)} {title}" if tags else title,
                        "link": url,
                        "summary": f"æ–°æµªä¸Šæµ· Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ–°æµªä¸Šæµ·",
                        "source": "æ–°æµªä¸Šæµ·",
                        "time": time_str[5:16] if len(time_str) > 16 else time_str,
                        "isNew": True,
                        "score": relevance['score']
                    })
                print(f"  âœ“ æ–°æµªä¸Šæµ·: {len(news_list)} æ¡")
    except Exception as e:
        print(f"  âœ— æ–°æµªä¸Šæµ·: {str(e)[:50]}")
    
    # 4. å°è¯• Google News ä¸Šæµ·
    print("\nğŸ” Google News ä¸Šæµ·")
    try:
        url = "https://news.google.com/rss/search?q=ä¸Šæµ·&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                if 'ä¸Šæµ·' in title:
                    items.append({
                        "title": title,
                        "link": entry.get("link", ""),
                        "summary": "Google News",
                        "source": "Google News",
                        "time": format_time(entry.get("published", "")),
                        "isNew": is_recent(entry.get("published_parsed")),
                        "score": 0
                    })
                    count += 1
            print(f"  âœ“ Google News: {count} æ¡")
    except Exception as e:
        print(f"  âœ— Google News: {str(e)[:50]}")
    
    # æŒ‰ç›¸å…³åº¦æ’åº
    items.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    return items

def fetch_tech_news():
    """æŠ“å–ç§‘æŠ€åª’ä½“ - TechCrunch"""
    items = []
    try:
        # TechCrunch RSS via RSSHub
        url = "https://rsshub.app/techcrunch"
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:8]:
                title = translate_text(html.unescape(entry.get("title", "")).strip())
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": "TechCrunch",
                    "source": "TechCrunch",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
        print(f"  âœ“ TechCrunch: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— TechCrunch: {str(e)[:50]}")
    return items

def fetch_github_trending():
    """æŠ“å– GitHub Trending"""
    items = []
    try:
        # GitHub Trending via RSSHub
        url = "https://rsshub.app/github/trending/daily/python"
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"â­ {title}",
                    "link": entry.get("link", ""),
                    "summary": "GitHub ä»Šæ—¥çƒ­é—¨",
                    "source": "GitHub",
                    "time": "ä»Šæ—¥",
                    "isNew": True
                })
        print(f"  âœ“ GitHub: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— GitHub: {str(e)[:50]}")
    return items

def fetch_bbc_news():
    """æŠ“å– BBC æ–°é—»"""
    items = []
    try:
        url = "http://feeds.bbci.co.uk/news/world/rss.xml"
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:8]:
                title = translate_text(html.unescape(entry.get("title", "")).strip())
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": "BBC World",
                    "source": "BBC",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
        print(f"  âœ“ BBC: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— BBC: {str(e)[:50]}")
    return items

def fetch_news():
    """ä¸»æŠ“å–å‡½æ•°"""
    print(f"\nâ° {datetime.now().strftime('%H:%M:%S')} - å¼€å§‹æŠ“å–...")
    
    news_data = {
        "shanghai": [],
        "stocks": [],
        "policy": [],
        "world": [],
        "ai": []
    }
    
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
    
    # 1. Reddit Worldnews
    print("\nğŸ”¥ REDDIT (å®æ—¶)")
    news_data["world"] = fetch_reddit_worldnews()
    
    # 2. BBC æ–°é—»
    print("\nğŸ“º BBC")
    bbc_news = fetch_bbc_news()
    news_data["world"].extend(bbc_news)
    
    # 3. Hacker News
    print("\nğŸ¤– HACKER NEWS")
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", 
                              timeout=10, proxies=PROXY)
        top_ids = response.json()[:10]
        
        for story_id in top_ids:
            try:
                story_resp = requests.get(
                    f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                    timeout=5, proxies=PROXY
                )
                story = story_resp.json()
                if story and story.get('title'):
                    translated_title = translate_text(story['title'])
                    
                    news_data["ai"].append({
                        "title": translated_title,
                        "link": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                        "summary": f"â­ {story.get('score', 0)} points",
                        "source": "Hacker News",
                        "time": datetime.now().strftime("%m-%d %H:%M"),
                        "isNew": True
                    })
            except:
                continue
        print(f"  âœ“ HN: {len(news_data['ai'])} æ¡")
    except Exception as e:
        print(f"  âœ— HN: {str(e)[:40]}")
    
    # 4. TechCrunch
    print("\nğŸ’» TECHCRUNCH")
    tech_news = fetch_tech_news()
    news_data["ai"].extend(tech_news)
    
    # 5. GitHub Trending
    print("\nğŸ™ GITHUB")
    github_news = fetch_github_trending()
    news_data["ai"].extend(github_news)
    
    # 6. ç¾è‚¡æ–°é—»
    print("\nğŸ“ˆ STOCKS")
    news_data["stocks"] = [
        {
            "title": "ğŸš€ RKLB Rocket Lab æœ€æ–°å‘å°„ä»»åŠ¡",
            "link": "https://www.rocketlabusa.com/news/",
            "summary": "å®˜æ–¹æ–°é—»ä¸å‘å°„æ›´æ–°",
            "source": "Rocket Lab",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True
        },
        {
            "title": "âš¡ TSLA ç‰¹æ–¯æ‹‰æŠ•èµ„è€…å…³ç³»",
            "link": "https://ir.tesla.com/",
            "summary": "è´¢æŠ¥ã€æ–°é—»ä¸å…¬å‘Š",
            "source": "Tesla IR",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True
        },
        {
            "title": "ğŸ”‹ QS QuantumScape æŠ€æœ¯è¿›å±•",
            "link": "https://www.quantumscape.com/news/",
            "summary": "å›ºæ€ç”µæ± ç ”å‘åŠ¨æ€",
            "source": "QuantumScape",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True
        },
        {
            "title": "ğŸ“Š PLTR Palantir å•†ä¸šåŠ¨æ€",
            "link": "https://investors.palantir.com/news/",
            "summary": "æ”¿åºœä¸ä¼ä¸šåˆåŒ",
            "source": "Palantir",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True
        },
        {
            "title": "ğŸ’Š RXRX Recursion AIè¯ç‰©ç ”å‘",
            "link": "https://www.recursion.com/news",
            "summary": "AIé©±åŠ¨çš„è¯ç‰©å‘ç°",
            "source": "Recursion",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True
        },
        {
            "title": "ğŸª™ BITGO åŠ å¯†æ‰˜ç®¡åŠ¨æ€",
            "link": "https://www.bitgo.com/news",
            "summary": "æ•°å­—èµ„äº§æ‰˜ç®¡æœåŠ¡",
            "source": "BitGo",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True
        },
    ]
    print(f"  âœ“ Stocks: {len(news_data['stocks'])} æ¡")
    
    # 7. ä¸Šæµ·æ–°é—» - æ¾æ¹ƒæ–°é—» RSS
    print("\nğŸ™ï¸ SHANGHAI")
    news_data["shanghai"] = fetch_shanghai_news()
    
    # 8. å›½å†…æ”¿ç­–
    print("\nğŸ‡¨ğŸ‡³ POLICY")
    news_data["policy"] = [
        {
            "title": "å›½åŠ¡é™¢å‘å¸ƒå…³äºæ¨åŠ¨æœªæ¥äº§ä¸šåˆ›æ–°å‘å±•çš„å®æ–½æ„è§",
            "link": "http://www.gov.cn",
            "summary": "å‰ç»å¸ƒå±€æœªæ¥äº§ä¸šï¼Œé‡ç‚¹æ¨è¿›å…­å¤§æ–¹å‘",
            "source": "ä¸­å›½æ”¿åºœç½‘",
            "time": "02-03",
            "isNew": True
        },
        {
            "title": "å·¥ä¿¡éƒ¨ï¼šåŠ å¿«åˆ¶é€ ä¸šæ•°å­—åŒ–è½¬å‹",
            "link": "https://www.miit.gov.cn",
            "summary": "æ¨åŠ¨åˆ¶é€ ä¸šé«˜ç«¯åŒ–ã€æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‘å±•",
            "source": "å·¥ä¿¡éƒ¨",
            "time": "02-02",
            "isNew": False
        },
        {
            "title": "å¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹",
            "link": "http://www.pbc.gov.cn",
            "summary": "é‡Šæ”¾é•¿æœŸèµ„é‡‘çº¦1ä¸‡äº¿å…ƒ",
            "source": "å¤®è¡Œ",
            "time": "02-01",
            "isNew": False
        },
    ]
    print(f"  âœ“ Policy: {len(news_data['policy'])} æ¡")
    
    # 9. ä¸ºæ–°é—»æ·»åŠ å°é¢å›¾ç‰‡ï¼ˆåªå¤„ç†å‰3æ¡ï¼Œé¿å…å¤ªæ…¢ï¼‰
    print("\nğŸ–¼ï¸ è·å–å°é¢å›¾ç‰‡...")
    for category, items in news_data.items():
        print(f"   {category}: ", end="", flush=True)
        for i, item in enumerate(items[:3]):  # åªå¤„ç†å‰3æ¡
            try:
                image_result = get_news_image(
                    title=item['title'],
                    url=item.get('link', ''),
                    category=category,
                    prefer_real=True
                )
                if image_result:
                    item['image'] = image_result['url']
                    item['imageType'] = image_result['type']  # 'real' æˆ– 'ai'
                else:
                    item['image'] = None
                    item['imageType'] = None
            except Exception as e:
                item['image'] = None
                item['imageType'] = None
        print(f"âœ“")
    
    # ä¿å­˜
    output_file = DATA_DIR / "news.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    
    # åŒæ—¶å¤åˆ¶åˆ°å‰ç«¯ç›®å½•
    import shutil
    shutil.copy(output_file, DATA_DIR.parent / "frontend" / "data.json")
    shutil.copy(output_file, DATA_DIR.parent / "data.json")
    
    print("\n" + "="*50)
    print(f"âœ… æ›´æ–°å®Œæˆ! æ€»è®¡: {sum(len(v) for v in news_data.values())} æ¡")
    print(f"   ä¸–ç•Œæ–°é—»: {len(news_data['world'])} æ¡ (Reddit + BBC)")
    print(f"   ä¸Šæµ·æ–°é—»: {len(news_data['shanghai'])} æ¡")
    print(f"   AI/Tech: {len(news_data['ai'])} æ¡ (HN + TechCrunch + GitHub)")
    print(f"   ç¾è‚¡: {len(news_data['stocks'])} æ¡")
    print(f"   æ”¿ç­–: {len(news_data['policy'])} æ¡")
    print(f"\nğŸ’¾ å·²ä¿å­˜")

if __name__ == "__main__":
    fetch_news()
