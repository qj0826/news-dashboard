#!/usr/bin/env python3
"""
æ–°é—»èšåˆæŠ“å–è„šæœ¬ - å®æ—¶ç‰ˆ
æ¯5åˆ†é’Ÿæ›´æ–°ï¼Œæ”¯æŒå…³é”®è¯æ¨é€
"""

import json
import requests
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import html
import urllib.parse
import sys

# å¯¼å…¥å›¾ç‰‡å¤„ç†æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from image_handler import get_news_image

DATA_DIR = Path(__file__).parent.parent / "data"
DATA_DIR.mkdir(exist_ok=True)

# ä»£ç†é…ç½®
PROXY = {'http': 'http://127.0.0.1:1082', 'https': 'http://127.0.0.1:1082'}

def translate_text(text, target_lang='zh-CN'):
    """ä½¿ç”¨ Google Translate å…è´¹ API ç¿»è¯‘"""
    try:
        if not text or len(text.strip()) == 0:
            return text
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return text  # å·²ç»æœ‰ä¸­æ–‡ï¼Œä¸ç¿»è¯‘
        
        # é™åˆ¶é•¿åº¦ï¼Œé¿å…APIé™åˆ¶
        text_to_translate = text[:500]
        
        # Google Translate å…è´¹ API
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl={target_lang}&dt=t&q={urllib.parse.quote(text_to_translate)}"
        
        response = requests.get(url, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            # è§£æè¿”å›ç»“æœ
            translated_parts = []
            for item in data[0]:
                if item[0]:
                    translated_parts.append(item[0])
            return ''.join(translated_parts)
        
        return text  # ç¿»è¯‘å¤±è´¥è¿”å›åŸæ–‡
    except Exception as e:
        return text  # å‡ºé”™è¿”å›åŸæ–‡

def format_time(published):
    """æ ¼å¼åŒ–æ—¶é—´"""
    if not published:
        return ""
    
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S GMT",
        "%Y-%m-%dT%H:%M:%S",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%d %H:%M:%S",
    ]
    
    for fmt in formats:
        try:
            clean_pub = published.strip().replace('+0800', '+08:00')
            dt = datetime.strptime(clean_pub[:26], fmt)
            return dt.strftime("%m-%d %H:%M")
        except:
            continue
    
    return published[:16] if len(published) > 16 else published

def is_recent(published_parsed):
    """åˆ¤æ–­æ˜¯å¦æ˜¯24å°æ—¶å†…çš„æ–°é—»"""
    if not published_parsed:
        return False
    try:
        from time import mktime
        pub_timestamp = mktime(published_parsed)
        pub_time = datetime.fromtimestamp(pub_timestamp)
        return datetime.now() - pub_time < timedelta(hours=24)
    except:
        return False

def fetch_reddit_worldnews():
    """æŠ“å– Reddit r/worldnews - æœ€å¿«"""
    items = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        url = "https://www.reddit.com/r/worldnews/new.json?limit=10"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            for post in data['data']['children']:
                post_data = post['data']
                title = html.unescape(post_data['title'])
                translated_title = translate_text(title)
                
                items.append({
                    "title": translated_title,
                    "link": "https://reddit.com" + post_data['permalink'],
                    "summary": f"â¬†ï¸ {post_data.get('score', 0)} | ğŸ’¬ {post_data.get('num_comments', 0)}",
                    "source": "Reddit å›½é™…æ–°é—»",
                    "time": datetime.fromtimestamp(post_data['created']).strftime("%m-%d %H:%M"),
                    "isNew": True
                })
        
        print(f"  âœ“ Reddit: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— Reddit: {str(e)[:50]}")
    
    return items

def is_shanghai_relevant(title, summary=""):
    """åˆ¤æ–­æ˜¯å¦ä¸å˜‰å®š/èŠ‚æ°”/ç¤¾åŒºç›¸å…³"""
    text = (title + summary).lower()
    
    # å˜‰å®šç›¸å…³
    jiading_keywords = ['å˜‰å®š', 'å—ç¿”', 'æ±Ÿæ¡¥', 'å®‰äº­', 'é©¬é™†', 'å¤–å†ˆ', 'å¾è¡Œ', 'åäº­', 'èŠå›­', 'æ–°æˆè·¯', 'çœŸæ–°', 'å˜‰å®šæ–°åŸ', 'å˜‰å®šå·¥ä¸šåŒº', 'å·æ¡¥', 'æ³•åå¡”']
    # èŠ‚æ°”æ—¶ä»¤
    season_keywords = ['ç«‹æ˜¥', 'é›¨æ°´', 'æƒŠè›°', 'æ˜¥åˆ†', 'æ¸…æ˜', 'è°·é›¨', 'ç«‹å¤', 'å°æ»¡', 'èŠ’ç§', 'å¤è‡³', 'å°æš‘', 'å¤§æš‘',
                       'ç«‹ç§‹', 'å¤„æš‘', 'ç™½éœ²', 'ç§‹åˆ†', 'å¯’éœ²', 'éœœé™', 'ç«‹å†¬', 'å°é›ª', 'å¤§é›ª', 'å†¬è‡³', 'å°å¯’', 'å¤§å¯’']
    # ç¤¾åŒºæ°‘ç”Ÿ
    community_keywords = ['ç¤¾åŒº', 'è¡—é“', 'å±…å§”ä¼š', 'ä¸šå§”ä¼š', 'ç‰©ä¸š', 'é‚»é‡Œ', 'ä¾¿æ°‘', 'ä¸ºè€', 'å…»è€', 'æ‰˜è‚²', 'èœåœº', 'æ—§æ”¹', 'åŠ è£…ç”µæ¢¯', 'é•¿æŠ¤é™©', 'åŒ»ä¿']
    
    has_jiading = any(kw in text for kw in jiading_keywords)
    has_season = any(kw in text for kw in season_keywords)
    has_community = any(kw in text for kw in community_keywords)
    
    return {
        'jiading': has_jiading,
        'season': has_season,
        'community': has_community,
        'score': int(has_jiading) * 3 + int(has_season) * 2 + int(has_community) * 1
    }

def fetch_shanghai_news():
    """æŠ“å–ä¸Šæµ·æ–°é—» - å¤šæºèšåˆ"""
    items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
    
    # 1. æ¾æ¹ƒæ–°é—» RSS
    try:
        url = "https://feedx.net/rss/thepaper.xml"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            
            for entry in feed.entries:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                # æ„å»ºæ ‡ç­¾
                tags = []
                if relevance['jiading']: tags.append('ğŸ ')
                if relevance['season']: tags.append('ğŸŒ¸')
                if relevance['community']: tags.append('ğŸ‘¥')
                
                # ä¼˜å…ˆæ·»åŠ ç›¸å…³æ–°é—»
                if relevance['score'] > 0 or len(items) < 20:
                    items.append({
                        "title": f"{' '.join(tags)} {title}" if tags else title,
                        "link": entry.get("link", ""),
                        "summary": f"æ¾æ¹ƒæ–°é—» Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ¾æ¹ƒæ–°é—» Â· ä¸Šæµ·",
                        "source": "æ¾æ¹ƒæ–°é—»",
                        "time": format_time(entry.get("published", "")),
                        "isNew": is_recent(entry.get("published_parsed")),
                        "score": relevance['score']
                    })
            
            print(f"  âœ“ æ¾æ¹ƒæ–°é—»: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— æ¾æ¹ƒæ–°é—»: {str(e)[:50]}")
    
    # 2. æ‰‹åŠ¨ç»´æŠ¤ - å˜‰å®šç²¾é€‰æ–°é—»
    print("\nğŸ  å˜‰å®šç²¾é€‰")
    jiading_manual = [
        {
            "title": "ğŸ  å˜‰å®šæ–°åŸå»ºè®¾æé€Ÿï¼Œå¤šä¸ªé‡å¤§é¡¹ç›®é›†ä¸­å¼€å·¥",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å˜‰å®šåŒºæ¨åŠ¨æ–°åŸå»ºè®¾ï¼Œèšç„¦ç§‘æŠ€åˆ›æ–°",
            "source": "å˜‰å®šå‘å¸ƒ",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 3
        },
        {
            "title": "ğŸ‘¥ å—ç¿”é•‡åŠ è£…ç”µæ¢¯å·¥ç¨‹åˆæœ‰æ–°è¿›å±•ï¼Œå¤šä¸ªå°åŒºå®Œæˆç­¾çº¦",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å—ç¿”é•‡æ¨è¿›è€æ—§å°åŒºåŠ è£…ç”µæ¢¯",
            "source": "å—ç¿”é•‡",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 3
        },
        {
            "title": "ğŸŒ¸ å·æ¡¥è€è¡—æ˜¥èŠ‚æ°‘ä¿—æ´»åŠ¨å®‰æ’å‡ºç‚‰",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å·æ¡¥è€è¡—æ˜¥èŠ‚æœŸé—´æ°‘ä¿—æ–‡åŒ–æ´»åŠ¨",
            "source": "å˜‰å®šæ–‡æ—…",
            "time": "02-01",
            "isNew": False,
            "score": 2
        },
        {
            "title": "ğŸ‘¥ æ±Ÿæ¡¥é•‡æ¨è¿›'15åˆ†é’Ÿç¤¾åŒºç”Ÿæ´»åœˆ'å»ºè®¾",
            "link": "https://www.jiading.gov.cn/",
            "summary": "æ±Ÿæ¡¥é•‡ä¾¿æ°‘æœåŠ¡è®¾æ–½å‡çº§",
            "source": "æ±Ÿæ¡¥é•‡",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 2
        },
    ]
    items.extend(jiading_manual)
    print(f"  âœ“ å˜‰å®šç²¾é€‰: {len(jiading_manual)} æ¡")
    
    # 3. æ–°æµªä¸Šæµ·æ–°é—»
    print("\nğŸ“° æ–°æµªä¸Šæµ·")
    try:
        url = "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2515&k=&num=20&r=0.123"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('result') and data['result'].get('data'):
                news_list = data['result']['data']
                for item in news_list:
                    title = item.get('title', '').strip()
                    url = item.get('url', '')
                    time_str = item.get('time', '')
                    
                    # æ£€æŸ¥ç›¸å…³æ€§
                    relevance = is_shanghai_relevant(title)
                    tags = []
                    if relevance['jiading']: tags.append('ğŸ ')
                    if relevance['season']: tags.append('ğŸŒ¸')
                    if relevance['community']: tags.append('ğŸ‘¥')
                    
                    items.append({
                        "title": f"{' '.join(tags)} {title}" if tags else title,
                        "link": url,
                        "summary": f"æ–°æµªä¸Šæµ· Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ–°æµªä¸Šæµ·",
                        "source": "æ–°æµªä¸Šæµ·",
                        "time": time_str[5:16] if len(time_str) > 16 else time_str,
                        "isNew": True,
                        "score": relevance['score']
                    })
                print(f"  âœ“ æ–°æµªä¸Šæµ·: {len(news_list)} æ¡")
    except Exception as e:
        print(f"  âœ— æ–°æµªä¸Šæµ·: {str(e)[:50]}")
    
    # 4. ä¸Šè§‚æ–°é—» (è§£æ”¾æ—¥æŠ¥)
    print("\nğŸ“° ä¸Šè§‚æ–°é—»")
    try:
        url = "https://rsshub.app/jfdaily/reconstruction"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:10]:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": f"ä¸Šè§‚æ–°é—» Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "ä¸Šè§‚æ–°é—»",
                    "source": "ä¸Šè§‚æ–°é—»",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed")),
                    "score": relevance['score']
                })
                count += 1
            print(f"  âœ“ ä¸Šè§‚æ–°é—»: {count} æ¡")
    except Exception as e:
        print(f"  âœ— ä¸Šè§‚æ–°é—»: {str(e)[:50]}")
    
    # 5. æ–‡æ±‡æŠ¥
    print("\nğŸ“° æ–‡æ±‡æŠ¥")
    try:
        url = "https://rsshub.app/whb/bihui"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:8]:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": f"æ–‡æ±‡æŠ¥ Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ–‡æ±‡æŠ¥",
                    "source": "æ–‡æ±‡æŠ¥",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed")),
                    "score": relevance['score']
                })
                count += 1
            print(f"  âœ“ æ–‡æ±‡æŠ¥: {count} æ¡")
    except Exception as e:
        print(f"  âœ— æ–‡æ±‡æŠ¥: {str(e)[:50]}")
    
    # 6. æ–°æ°‘æ™šæŠ¥
    print("\nğŸ“° æ–°æ°‘æ™šæŠ¥")
    try:
        url = "https://rsshub.app/xinmin/daily"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:8]:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": f"æ–°æ°‘æ™šæŠ¥ Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ–°æ°‘æ™šæŠ¥",
                    "source": "æ–°æ°‘æ™šæŠ¥",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed")),
                    "score": relevance['score']
                })
                count += 1
            print(f"  âœ“ æ–°æ°‘æ™šæŠ¥: {count} æ¡")
    except Exception as e:
        print(f"  âœ— æ–°æ°‘æ™šæŠ¥: {str(e)[:50]}")
    
    # 7. ä¸œæ–¹ç½‘ - ä½¿ç”¨å¤‡ç”¨ API
    print("\nğŸ“° ä¸œæ–¹ç½‘")
    try:
        # å°è¯•å¤šä¸ªä¸œæ–¹ç½‘æ•°æ®æº
        urls_to_try = [
            "https://rsshub.app/eastday/sh",
            "https://rsshub.app/eastday/china",
        ]
        
        for url in urls_to_try:
            try:
                response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
                if response.status_code == 200:
                    feed = feedparser.parse(response.content)
                    count = 0
                    for entry in feed.entries[:8]:
                        title = html.unescape(entry.get("title", "")).strip()
                        relevance = is_shanghai_relevant(title)
                        
                        items.append({
                            "title": title,
                            "link": entry.get("link", ""),
                            "summary": f"ä¸œæ–¹ç½‘ Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "ä¸œæ–¹ç½‘",
                            "source": "ä¸œæ–¹ç½‘",
                            "time": format_time(entry.get("published", "")),
                            "isNew": is_recent(entry.get("published_parsed")),
                            "score": relevance['score']
                        })
                        count += 1
                    print(f"  âœ“ ä¸œæ–¹ç½‘: {count} æ¡")
                    break
            except:
                continue
    except Exception as e:
        print(f"  âœ— ä¸œæ–¹ç½‘: {str(e)[:50]}")
    
    # 8. çœ‹çœ‹æ–°é—» - ç›´æ¥æŠ“å–ç½‘é¡µ
    print("\nğŸ“º çœ‹çœ‹æ–°é—»")
    try:
        url = "https://www.kankanews.com/"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            # æå–æ–°é—»é“¾æ¥å’Œæ ‡é¢˜
            import re
            # åŒ¹é…çœ‹çœ‹æ–°é—»çš„é“¾æ¥æ¨¡å¼ /a/YYYY-MM-DD/xxxxx.shtml
            news_pattern = r'href="(/a/\d{4}-\d{2}-\d{2}/\d+\.shtml)"[^>]*>([^<]+)</a>'
            matches = re.findall(news_pattern, response.text)
            
            seen = set()
            count = 0
            for link, title in matches[:10]:
                if link not in seen and title.strip():
                    seen.add(link)
                    full_link = f"https://www.kankanews.com{link}" if link.startswith('/') else link
                    title_clean = html.unescape(title.strip())
                    relevance = is_shanghai_relevant(title_clean)
                    
                    items.append({
                        "title": title_clean,
                        "link": full_link,
                        "summary": f"çœ‹çœ‹æ–°é—» Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "çœ‹çœ‹æ–°é—»",
                        "source": "çœ‹çœ‹æ–°é—»",
                        "time": datetime.now().strftime("%m-%d"),
                        "isNew": True,
                        "score": relevance['score']
                    })
                    count += 1
            
            print(f"  âœ“ çœ‹çœ‹æ–°é—»: {count} æ¡")
    except Exception as e:
        print(f"  âœ— çœ‹çœ‹æ–°é—»: {str(e)[:50]}")
    
    # 9. æ–°é—»æ™¨æŠ¥
    print("\nğŸ“° æ–°é—»æ™¨æŠ¥")
    try:
        url = "https://rsshub.app/shxwcb"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:8]:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": f"æ–°é—»æ™¨æŠ¥ Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ–°é—»æ™¨æŠ¥",
                    "source": "æ–°é—»æ™¨æŠ¥",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed")),
                    "score": relevance['score']
                })
                count += 1
            print(f"  âœ“ æ–°é—»æ™¨æŠ¥: {count} æ¡")
    except Exception as e:
        print(f"  âœ— æ–°é—»æ™¨æŠ¥: {str(e)[:50]}")
    
    # 10. é’å¹´æŠ¥
    print("\nğŸ“° é’å¹´æŠ¥")
    try:
        url = "https://rsshub.app/qnb"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:8]:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": f"é’å¹´æŠ¥ Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "é’å¹´æŠ¥",
                    "source": "é’å¹´æŠ¥",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed")),
                    "score": relevance['score']
                })
                count += 1
            print(f"  âœ“ é’å¹´æŠ¥: {count} æ¡")
    except Exception as e:
        print(f"  âœ— é’å¹´æŠ¥: {str(e)[:50]}")
    
    # 11. åŠ³åŠ¨æŠ¥
    print("\nğŸ“° åŠ³åŠ¨æŠ¥")
    try:
        url = "https://rsshub.app/ldrb"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:8]:
                title = html.unescape(entry.get("title", "")).strip()
                relevance = is_shanghai_relevant(title)
                
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": f"åŠ³åŠ¨æŠ¥ Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "åŠ³åŠ¨æŠ¥",
                    "source": "åŠ³åŠ¨æŠ¥",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed")),
                    "score": relevance['score']
                })
                count += 1
            print(f"  âœ“ åŠ³åŠ¨æŠ¥: {count} æ¡")
    except Exception as e:
        print(f"  âœ— åŠ³åŠ¨æŠ¥: {str(e)[:50]}")
    
    # æŒ‰ç›¸å…³åº¦æ’åº
    items.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    return items

def fetch_us_stock_news():
    """æŠ“å–ç¾è‚¡æ–°é—» - å¤šæºèšåˆ"""
    items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
    
    # æŒä»“è‚¡ç¥¨åˆ—è¡¨
    portfolio = ['TSLA', 'RKLB', 'QS', 'PLTR', 'RXRX', 'COIN', 'MSTR']
    
    # 1. Finnhub ç¾è‚¡æ–°é—»ï¼ˆå…è´¹APIï¼‰
    try:
        # Finnhub å…è´¹ç‰ˆä¸éœ€è¦API keyä¹Ÿèƒ½è·å–éƒ¨åˆ†æ–°é—»
        url = "https://finnhub.io/api/v1/news?category=general"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            news_list = response.json()
            for news in news_list[:10]:
                title = news.get('headline', '')
                # æ£€æŸ¥æ˜¯å¦ä¸æŒä»“ç›¸å…³
                related_symbols = [s for s in portfolio if s in str(news.get('related', ''))]
                symbol_tag = f"[{','.join(related_symbols)}] " if related_symbols else ""
                
                items.append({
                    "title": f"{symbol_tag}{title}",
                    "link": news.get('url', ''),
                    "summary": news.get('source', 'Finnhub'),
                    "source": "ç¾è‚¡å¿«è®¯",
                    "time": datetime.fromtimestamp(news.get('datetime', 0)).strftime("%m-%d %H:%M") if news.get('datetime') else datetime.now().strftime("%m-%d"),
                    "isNew": True
                })
            print(f"    âœ“ Finnhub: {len(items)} æ¡")
    except Exception as e:
        print(f"    âœ— Finnhub: {str(e)[:40]}")
    
    # 2. Yahoo Finance RSSï¼ˆå¸‚åœºæ–°é—»ï¼‰
    try:
        url = "https://rsshub.app/yahoo/news/markets"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:8]:
                title = translate_text(html.unescape(entry.get("title", "")).strip())
                
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": "Yahoo Finance",
                    "source": "Yahooè´¢ç»",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"    âœ“ Yahoo Finance: {count} æ¡")
    except Exception as e:
        print(f"    âœ— Yahoo Finance: {str(e)[:40]}")
    
    # 3. Seeking Alpha çƒ­é—¨
    try:
        url = "https://rsshub.app/seekingalpha/feed/top-news"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:6]:
                title = html.unescape(entry.get("title", "")).strip()
                # æ£€æŸ¥æ˜¯å¦ä¸æŒä»“ç›¸å…³
                related = any(s.lower() in title.lower() for s in portfolio)
                prefix = "ğŸ“ˆ " if related else ""
                
                items.append({
                    "title": f"{prefix}{title}",
                    "link": entry.get("link", ""),
                    "summary": "Seeking Alpha",
                    "source": "Seeking Alpha",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"    âœ“ Seeking Alpha: {count} æ¡")
    except Exception as e:
        print(f"    âœ— Seeking Alpha: {str(e)[:40]}")
    
    # 4. å¦‚æœä¸Šé¢éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é™æ€é“¾æ¥
    if len(items) < 5:
        backup_items = [
            {
                "title": "ğŸš€ RKLB Rocket Lab æœ€æ–°å‘å°„ä»»åŠ¡",
                "link": "https://www.rocketlabusa.com/news/",
                "summary": "å®˜æ–¹æ–°é—»ä¸å‘å°„æ›´æ–°",
                "source": "Rocket Lab",
                "time": datetime.now().strftime("%m-%d"),
                "isNew": True
            },
            {
                "title": "âš¡ TSLA ç‰¹æ–¯æ‹‰æŠ•èµ„è€…å…³ç³»",
                "link": "https://ir.tesla.com/",
                "summary": "è´¢æŠ¥ã€æ–°é—»ä¸å…¬å‘Š",
                "source": "Tesla IR",
                "time": datetime.now().strftime("%m-%d"),
                "isNew": True
            },
            {
                "title": "ğŸ“Š PLTR Palantir å•†ä¸šåŠ¨æ€",
                "link": "https://investors.palantir.com/news/",
                "summary": "æ”¿åºœä¸ä¼ä¸šåˆåŒ",
                "source": "Palantir",
                "time": datetime.now().strftime("%m-%d"),
                "isNew": True
            },
        ]
        items.extend(backup_items)
        print(f"    âš ï¸ ä½¿ç”¨å¤‡ç”¨æ•°æ®: {len(backup_items)} æ¡")
    
    # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
    seen = set()
    unique_items = []
    for item in items:
        if item['title'] not in seen:
            seen.add(item['title'])
            unique_items.append(item)
    
    return unique_items[:15]  # æœ€å¤šè¿”å›15æ¡

def fetch_ai_news():
    """æŠ“å–AI/Techæ–°é—» - å¤šæºèšåˆ"""
    items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
    
    # 1. TechCrunch AI/ç§‘æŠ€æ–°é—»
    print("\nğŸ’» TECHCRUNCH")
    try:
        url = "https://rsshub.app/techcrunch"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:6]:
                title = translate_text(html.unescape(entry.get("title", "")).strip())
                items.append({
                    "title": f"ğŸš€ {title}",
                    "link": entry.get("link", ""),
                    "summary": "TechCrunch",
                    "source": "TechCrunch",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ TechCrunch: {count} æ¡")
    except Exception as e:
        print(f"  âœ— TechCrunch: {str(e)[:50]}")
    
    # 2. OpenAI åšå®¢
    print("\nğŸ¤– OPENAI")
    try:
        url = "https://rsshub.app/openai/blog"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ”¥ {title}",
                    "link": entry.get("link", ""),
                    "summary": "OpenAI å®˜æ–¹",
                    "source": "OpenAI",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ OpenAI: {count} æ¡")
    except Exception as e:
        print(f"  âœ— OpenAI: {str(e)[:50]}")
    
    # 3. Google AI åšå®¢
    print("\nğŸ§  GOOGLE AI")
    try:
        url = "https://rsshub.app/google/research"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ”¬ {title}",
                    "link": entry.get("link", ""),
                    "summary": "Google Research",
                    "source": "Google AI",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ Google AI: {count} æ¡")
    except Exception as e:
        print(f"  âœ— Google AI: {str(e)[:50]}")
    
    # 4. Papers With Code æœ€æ–°è®ºæ–‡
    print("\nğŸ“„ PAPERS WITH CODE")
    try:
        url = "https://rsshub.app/papers/arxiv/CS.AI"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ“„ {title[:60]}...",
                    "link": entry.get("link", ""),
                    "summary": "arXiv AI",
                    "source": "arXiv",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ arXiv AI: {count} æ¡")
    except Exception as e:
        print(f"  âœ— arXiv: {str(e)[:50]}")
    
    # 5. GitHub Trending
    print("\nğŸ™ GITHUB TRENDING")
    try:
        url = "https://rsshub.app/github/trending/daily/python"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"â­ {title}",
                    "link": entry.get("link", ""),
                    "summary": "GitHub ä»Šæ—¥çƒ­é—¨",
                    "source": "GitHub",
                    "time": "ä»Šæ—¥",
                    "isNew": True
                })
                count += 1
            print(f"  âœ“ GitHub: {count} æ¡")
    except Exception as e:
        print(f"  âœ— GitHub: {str(e)[:50]}")
    
    # å»é‡
    seen = set()
    unique_items = []
    for item in items:
        if item['title'] not in seen:
            seen.add(item['title'])
            unique_items.append(item)
    
    return unique_items[:20]

def fetch_tech_news():
    """å…¼å®¹æ—§å‡½æ•° - è°ƒç”¨æ–°çš„AIæŠ“å–"""
    return fetch_ai_news()

def fetch_policy_news():
    """æŠ“å–æ”¿ç­–æ–°é—» - å›½åŠ¡é™¢ã€å„éƒ¨å§”ã€ä¸Šæµ·å¸‚æ”¿åºœ"""
    items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
    
    # 1. ä¸­å›½æ”¿åºœç½‘ - å›½åŠ¡é™¢æ”¿ç­–
    print("\nğŸ›ï¸ ä¸­å›½æ”¿åºœç½‘")
    try:
        url = "https://rsshub.app/gov/zhengce/zuixin"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:8]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ‡¨ğŸ‡³ {title}",
                    "link": entry.get("link", ""),
                    "summary": "å›½åŠ¡é™¢æ”¿ç­–",
                    "source": "å›½åŠ¡é™¢",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ å›½åŠ¡é™¢: {count} æ¡")
    except Exception as e:
        print(f"  âœ— å›½åŠ¡é™¢: {str(e)[:50]}")
    
    # 2. å›½å®¶å‘æ”¹å§”
    print("\nğŸ“Š å›½å®¶å‘æ”¹å§”")
    try:
        url = "https://rsshub.app/gov/ndrc/zwxxgk"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ“ˆ {title}",
                    "link": entry.get("link", ""),
                    "summary": "å›½å®¶å‘æ”¹å§”",
                    "source": "å‘æ”¹å§”",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ å‘æ”¹å§”: {count} æ¡")
    except Exception as e:
        print(f"  âœ— å‘æ”¹å§”: {str(e)[:50]}")
    
    # 3. å·¥ä¿¡éƒ¨
    print("\nğŸ”§ å·¥ä¿¡éƒ¨")
    try:
        url = "https://rsshub.app/gov/miit/zcwj"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ”§ {title}",
                    "link": entry.get("link", ""),
                    "summary": "å·¥ä¿¡éƒ¨æ”¿ç­–",
                    "source": "å·¥ä¿¡éƒ¨",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ å·¥ä¿¡éƒ¨: {count} æ¡")
    except Exception as e:
        print(f"  âœ— å·¥ä¿¡éƒ¨: {str(e)[:50]}")
    
    # 4. å¤®è¡Œ
    print("\nğŸ¦ å¤®è¡Œ")
    try:
        url = "https://rsshub.app/gov/pbc/zcyj"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ’° {title}",
                    "link": entry.get("link", ""),
                    "summary": "å¤®è¡Œæ”¿ç­–ç ”ç©¶",
                    "source": "å¤®è¡Œ",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ å¤®è¡Œ: {count} æ¡")
    except Exception as e:
        print(f"  âœ— å¤®è¡Œ: {str(e)[:50]}")
    
    # 5. ä¸Šæµ·å¸‚æ”¿åºœ
    print("\nğŸ™ï¸ ä¸Šæµ·å¸‚æ”¿åºœ")
    try:
        url = "https://rsshub.app/gov/shanghai/zhengce"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸ™ï¸ {title}",
                    "link": entry.get("link", ""),
                    "summary": "ä¸Šæµ·å¸‚æ”¿åºœ",
                    "source": "ä¸Šæµ·å¸‚æ”¿åºœ",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ ä¸Šæµ·å¸‚æ”¿åºœ: {count} æ¡")
    except Exception as e:
        print(f"  âœ— ä¸Šæµ·å¸‚æ”¿åºœ: {str(e)[:50]}")
    
    # 6. å•†åŠ¡éƒ¨
    print("\nğŸŒ å•†åŠ¡éƒ¨")
    try:
        url = "https://rsshub.app/gov/mofcom/swgat"
        response = requests.get(url, headers=headers, timeout=10, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            count = 0
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"ğŸŒ {title}",
                    "link": entry.get("link", ""),
                    "summary": "å•†åŠ¡éƒ¨",
                    "source": "å•†åŠ¡éƒ¨",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
                count += 1
            print(f"  âœ“ å•†åŠ¡éƒ¨: {count} æ¡")
    except Exception as e:
        print(f"  âœ— å•†åŠ¡éƒ¨: {str(e)[:50]}")
    
    # å»é‡
    seen = set()
    unique_items = []
    for item in items:
        if item['title'] not in seen:
            seen.add(item['title'])
            unique_items.append(item)
    
    return unique_items[:20]

def fetch_github_trending():
    """æŠ“å– GitHub Trending"""
    items = []
    try:
        # GitHub Trending via RSSHub
        url = "https://rsshub.app/github/trending/daily/python"
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:5]:
                title = html.unescape(entry.get("title", "")).strip()
                items.append({
                    "title": f"â­ {title}",
                    "link": entry.get("link", ""),
                    "summary": "GitHub ä»Šæ—¥çƒ­é—¨",
                    "source": "GitHub",
                    "time": "ä»Šæ—¥",
                    "isNew": True
                })
        print(f"  âœ“ GitHub: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— GitHub: {str(e)[:50]}")
    return items

def fetch_bbc_news():
    """æŠ“å– BBC æ–°é—»"""
    items = []
    try:
        url = "http://feeds.bbci.co.uk/news/world/rss.xml"
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:8]:
                title = translate_text(html.unescape(entry.get("title", "")).strip())
                items.append({
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": "BBC World",
                    "source": "BBC",
                    "time": format_time(entry.get("published", "")),
                    "isNew": is_recent(entry.get("published_parsed"))
                })
        print(f"  âœ“ BBC: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— BBC: {str(e)[:50]}")
    return items

def fetch_news():
    """ä¸»æŠ“å–å‡½æ•°"""
    print(f"\nâ° {datetime.now().strftime('%H:%M:%S')} - å¼€å§‹æŠ“å–...")
    
    news_data = {
        "shanghai": [],
        "stocks": [],
        "policy": [],
        "world": [],
        "ai": []
    }
    
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
    
    # 1. Reddit Worldnews
    print("\nğŸ”¥ REDDIT (å®æ—¶)")
    news_data["world"] = fetch_reddit_worldnews()
    
    # 2. BBC æ–°é—»
    print("\nğŸ“º BBC")
    bbc_news = fetch_bbc_news()
    news_data["world"].extend(bbc_news)
    
    # 3. Hacker News
    print("\nğŸ¤– HACKER NEWS")
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", 
                              timeout=10, proxies=PROXY)
        top_ids = response.json()[:10]
        
        for story_id in top_ids:
            try:
                story_resp = requests.get(
                    f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                    timeout=5, proxies=PROXY
                )
                story = story_resp.json()
                if story and story.get('title'):
                    translated_title = translate_text(story['title'])
                    
                    news_data["ai"].append({
                        "title": translated_title,
                        "link": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                        "summary": f"â­ {story.get('score', 0)} points",
                        "source": "Hacker News",
                        "time": datetime.now().strftime("%m-%d %H:%M"),
                        "isNew": True
                    })
            except:
                continue
        print(f"  âœ“ HN: {len(news_data['ai'])} æ¡")
    except Exception as e:
        print(f"  âœ— HN: {str(e)[:40]}")
    
    # 4. AI/Tech ç»¼åˆæ–°é—»
    print("\nğŸ¤– AI/TECH ç»¼åˆ")
    ai_news = fetch_ai_news()
    news_data["ai"] = ai_news
    
    # 6. ç¾è‚¡æ–°é—» - å®æ—¶æŠ“å–å¤šæº
    print("\nğŸ“ˆ STOCKS")
    news_data["stocks"] = fetch_us_stock_news()
    print(f"  âœ“ Stocks: {len(news_data['stocks'])} æ¡")
    
    # 7. ä¸Šæµ·æ–°é—» - æ¾æ¹ƒæ–°é—» RSS
    print("\nğŸ™ï¸ SHANGHAI")
    news_data["shanghai"] = fetch_shanghai_news()
    
    # 8. å›½å†…æ”¿ç­– - å¤šæºå®æ—¶æŠ“å–
    print("\nğŸ‡¨ğŸ‡³ POLICY")
    news_data["policy"] = fetch_policy_news()
    print(f"  âœ“ Policy: {len(news_data['policy'])} æ¡")
    
    # 9. ä¸ºæ–°é—»æ·»åŠ å°é¢å›¾ç‰‡ï¼ˆåªå¤„ç†å‰3æ¡ï¼Œé¿å…å¤ªæ…¢ï¼‰
    print("\nğŸ–¼ï¸ è·å–å°é¢å›¾ç‰‡...")
    for category, items in news_data.items():
        print(f"   {category}: ", end="", flush=True)
        for i, item in enumerate(items[:3]):  # åªå¤„ç†å‰3æ¡
            try:
                image_result = get_news_image(
                    title=item['title'],
                    url=item.get('link', ''),
                    category=category,
                    prefer_real=True
                )
                if image_result:
                    item['image'] = image_result['url']
                    item['imageType'] = image_result['type']  # 'real' æˆ– 'ai'
                else:
                    item['image'] = None
                    item['imageType'] = None
            except Exception as e:
                item['image'] = None
                item['imageType'] = None
        print(f"âœ“")
    
    # ä¿å­˜
    output_file = DATA_DIR / "news.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    
    # åŒæ—¶å¤åˆ¶åˆ°å‰ç«¯ç›®å½•
    import shutil
    shutil.copy(output_file, DATA_DIR.parent / "frontend" / "data.json")
    shutil.copy(output_file, DATA_DIR.parent / "data.json")
    
    print("\n" + "="*50)
    print(f"âœ… æ›´æ–°å®Œæˆ! æ€»è®¡: {sum(len(v) for v in news_data.values())} æ¡")
    print(f"   ä¸–ç•Œæ–°é—»: {len(news_data['world'])} æ¡ (Reddit + BBC)")
    print(f"   ä¸Šæµ·æ–°é—»: {len(news_data['shanghai'])} æ¡")
    print(f"   AI/Tech: {len(news_data['ai'])} æ¡ (HN + TechCrunch + GitHub)")
    print(f"   ç¾è‚¡: {len(news_data['stocks'])} æ¡")
    print(f"   æ”¿ç­–: {len(news_data['policy'])} æ¡")
    print(f"\nğŸ’¾ å·²ä¿å­˜")

if __name__ == "__main__":
    fetch_news()
