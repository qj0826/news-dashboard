#!/usr/bin/env python3
"""
ä¸Šæµ·æ–°é—»æŠ“å– - ç›´æ¥æŠ“å–å®˜ç½‘ï¼ˆç»•è¿‡RSSHubï¼‰
"""

import requests
import re
import json
from datetime import datetime
from html import unescape

PROXY = {'http': 'http://127.0.0.1:1082', 'https': 'http://127.0.0.1:1082'}
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}

def is_shanghai_relevant(title, summary=""):
    """åˆ¤æ–­æ˜¯å¦ä¸å˜‰å®š/èŠ‚æ°”/ç¤¾åŒºç›¸å…³"""
    text = (title + summary).lower()
    
    jiading_keywords = ['å˜‰å®š', 'å—ç¿”', 'æ±Ÿæ¡¥', 'å®‰äº­', 'é©¬é™†', 'å¤–å†ˆ', 'å¾è¡Œ', 'åäº­', 'èŠå›­', 'æ–°æˆè·¯', 'çœŸæ–°', 'å˜‰å®šæ–°åŸ', 'å·æ¡¥', 'æ³•åå¡”']
    season_keywords = ['ç«‹æ˜¥', 'é›¨æ°´', 'æƒŠè›°', 'æ˜¥åˆ†', 'æ¸…æ˜', 'è°·é›¨', 'ç«‹å¤', 'å°æ»¡', 'èŠ’ç§', 'å¤è‡³', 'å°æš‘', 'å¤§æš‘',
                       'ç«‹ç§‹', 'å¤„æš‘', 'ç™½éœ²', 'ç§‹åˆ†', 'å¯’éœ²', 'éœœé™', 'ç«‹å†¬', 'å°é›ª', 'å¤§é›ª', 'å†¬è‡³', 'å°å¯’', 'å¤§å¯’']
    community_keywords = ['ç¤¾åŒº', 'è¡—é“', 'å±…å§”ä¼š', 'ä¸šå§”ä¼š', 'ç‰©ä¸š', 'é‚»é‡Œ', 'ä¾¿æ°‘', 'å…»è€', 'æ‰˜è‚²', 'èœåœº', 'æ—§æ”¹', 'åŠ è£…ç”µæ¢¯']
    
    has_jiading = any(kw in text for kw in jiading_keywords)
    has_season = any(kw in text for kw in season_keywords)
    has_community = any(kw in text for kw in community_keywords)
    
    return {
        'jiading': has_jiading,
        'season': has_season,
        'community': has_community,
        'score': int(has_jiading) * 3 + int(has_season) * 2 + int(has_community) * 1
    }

def fetch_thepaper():
    """æŠ“å–æ¾æ¹ƒæ–°é—»å®˜ç½‘"""
    items = []
    try:
        # ç›´æ¥æŠ“é¦–é¡µçƒ­é—¨æ–°é—»
        url = "https://www.thepaper.cn/"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            html = response.text
            # å°è¯•å¤šç§åŒ¹é…æ¨¡å¼
            patterns = [
                r'href="(/newsDetail_forward_\d+)"[^>]*>[^<]*<[^>]*>([^<]{10,100})</',
                r'href="(/newsDetail_forward_\d+)"[^>]*>\s*([^<]{10,100})',
            ]
            
            seen = set()
            for pattern in patterns:
                matches = re.findall(pattern, html)
                for link, title in matches[:15]:
                    if link not in seen and title.strip() and len(title.strip()) > 10:
                        seen.add(link)
                        title_clean = unescape(title.strip())
                        relevance = is_shanghai_relevant(title_clean)
                        
                        tags = []
                        if relevance['jiading']: tags.append('ğŸ ')
                        if relevance['season']: tags.append('ğŸŒ¸')
                        if relevance['community']: tags.append('ğŸ‘¥')
                        
                        items.append({
                            "title": f"{' '.join(tags)} {title_clean}" if tags else title_clean,
                            "link": f"https://www.thepaper.cn{link}",
                            "summary": "æ¾æ¹ƒæ–°é—»",
                            "source": "æ¾æ¹ƒæ–°é—»",
                            "time": datetime.now().strftime("%m-%d"),
                            "isNew": True,
                            "score": relevance['score']
                        })
                if len(items) > 0:
                    break
        
        print(f"  âœ“ æ¾æ¹ƒæ–°é—»: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— æ¾æ¹ƒæ–°é—»: {str(e)[:50]}")
    
    return items

def fetch_eastday():
    """æŠ“å–ä¸œæ–¹ç½‘"""
    items = []
    try:
        url = "https://www.eastday.com/"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            html = response.text
            # åŒ¹é…æ–°é—»æ ‡é¢˜å’Œé“¾æ¥
            pattern = r'href="(https?://[^"]*eastday[^"]*/[\d/]+[^"]*)"[^>]*>([^<]{10,})<'
            matches = re.findall(pattern, html)
            
            seen = set()
            for link, title in matches[:10]:
                if link not in seen and title.strip():
                    seen.add(link)
                    title_clean = unescape(title.strip())
                    relevance = is_shanghai_relevant(title_clean)
                    
                    items.append({
                        "title": title_clean,
                        "link": link,
                        "summary": f"ä¸œæ–¹ç½‘ Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "ä¸œæ–¹ç½‘",
                        "source": "ä¸œæ–¹ç½‘",
                        "time": datetime.now().strftime("%m-%d"),
                        "isNew": True,
                        "score": relevance['score']
                    })
        
        print(f"  âœ“ ä¸œæ–¹ç½‘: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— ä¸œæ–¹ç½‘: {str(e)[:50]}")
    
    return items

def fetch_sina_shanghai():
    """æŠ“å–æ–°æµªä¸Šæµ·æ–°é—»"""
    items = []
    try:
        url = "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2515&k=&num=20&r=0.123"
        response = requests.get(url, headers=headers, timeout=15, proxies=PROXY)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('result') and data['result'].get('data'):
                news_list = data['result']['data']
                for item in news_list:
                    title = item.get('title', '').strip()
                    url = item.get('url', '')
                    time_str = item.get('time', '')
                    
                    relevance = is_shanghai_relevant(title)
                    tags = []
                    if relevance['jiading']: tags.append('ğŸ ')
                    if relevance['season']: tags.append('ğŸŒ¸')
                    if relevance['community']: tags.append('ğŸ‘¥')
                    
                    items.append({
                        "title": f"{' '.join(tags)} {title}" if tags else title,
                        "link": url,
                        "summary": f"æ–°æµªä¸Šæµ· Â· ç›¸å…³åº¦:{relevance['score']}" if relevance['score'] > 0 else "æ–°æµªä¸Šæµ·",
                        "source": "æ–°æµªä¸Šæµ·",
                        "time": time_str[5:16] if len(time_str) > 16 else time_str,
                        "isNew": True,
                        "score": relevance['score']
                    })
        
        print(f"  âœ“ æ–°æµªä¸Šæµ·: {len(items)} æ¡")
    except Exception as e:
        print(f"  âœ— æ–°æµªä¸Šæµ·: {str(e)[:50]}")
    
    return items

def fetch_shanghai_news():
    """ä¸»æŠ“å–å‡½æ•°"""
    items = []
    
    print("\n=== æŠ“å–ä¸Šæµ·æ–°é—» ===")
    
    # 1. æ¾æ¹ƒæ–°é—»ï¼ˆå®˜ç½‘æŠ“å–ï¼‰
    items.extend(fetch_thepaper())
    
    # 2. ä¸œæ–¹ç½‘
    items.extend(fetch_eastday())
    
    # 3. æ–°æµªä¸Šæµ·
    items.extend(fetch_sina_shanghai())
    
    # 4. å˜‰å®šç²¾é€‰ï¼ˆæ‰‹åŠ¨ç»´æŠ¤ï¼‰
    print("\nğŸ  å˜‰å®šç²¾é€‰")
    jiading_manual = [
        {
            "title": "ğŸ  å˜‰å®šæ–°åŸå»ºè®¾æé€Ÿï¼Œå¤šä¸ªé‡å¤§é¡¹ç›®é›†ä¸­å¼€å·¥",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å˜‰å®šåŒºæ¨åŠ¨æ–°åŸå»ºè®¾ï¼Œèšç„¦ç§‘æŠ€åˆ›æ–°",
            "source": "å˜‰å®šå‘å¸ƒ",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 3
        },
        {
            "title": "ğŸ‘¥ å—ç¿”é•‡åŠ è£…ç”µæ¢¯å·¥ç¨‹åˆæœ‰æ–°è¿›å±•ï¼Œå¤šä¸ªå°åŒºå®Œæˆç­¾çº¦",
            "link": "https://www.jiading.gov.cn/",
            "summary": "å—ç¿”é•‡æ¨è¿›è€æ—§å°åŒºåŠ è£…ç”µæ¢¯",
            "source": "å—ç¿”é•‡",
            "time": datetime.now().strftime("%m-%d"),
            "isNew": True,
            "score": 3
        },
    ]
    items.extend(jiading_manual)
    print(f"  âœ“ å˜‰å®šç²¾é€‰: {len(jiading_manual)} æ¡")
    
    # å»é‡
    seen = set()
    unique_items = []
    for item in items:
        if item['title'] not in seen:
            seen.add(item['title'])
            unique_items.append(item)
    
    # æŒ‰ç›¸å…³åº¦æ’åº
    unique_items.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    print(f"\nâœ… ä¸Šæµ·æ–°é—»æ€»è®¡: {len(unique_items)} æ¡")
    return unique_items

if __name__ == '__main__':
    news = fetch_shanghai_news()
    print(f"\næ¥æºç»Ÿè®¡:")
    sources = {}
    for item in news:
        src = item['source']
        sources[src] = sources.get(src, 0) + 1
    for src, count in sources.items():
        print(f"  {src}: {count}æ¡")
